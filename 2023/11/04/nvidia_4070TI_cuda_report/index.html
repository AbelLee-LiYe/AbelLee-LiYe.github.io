<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta http-equiv="x-ua-compatible" content="ie=edge"/><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"/><style data-href="/styles.9411df6cd9768a96fee2.css" id="gatsby-global-css">/*! normalize.css v8.0.1 | MIT License | github.com/necolas/normalize.css */html{line-height:1.15;-webkit-text-size-adjust:100%}body{margin:0}main{display:block}h1{font-size:2em;margin:.67em 0}hr{box-sizing:content-box;height:0;overflow:visible}pre{font-family:monospace,monospace;font-size:1em}a{background-color:transparent}abbr[title]{border-bottom:none;text-decoration:underline;text-decoration:underline dotted}b,strong{font-weight:bolder}code,kbd,samp{font-family:monospace,monospace;font-size:1em}small{font-size:80%}sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}sub{bottom:-.25em}sup{top:-.5em}img{border-style:none}button,input,optgroup,select,textarea{font-family:inherit;font-size:100%;line-height:1.15;margin:0}button,input{overflow:visible}button,select{text-transform:none}[type=button],button{-webkit-appearance:button}[type=button]::-moz-focus-inner,button::-moz-focus-inner{border-style:none;padding:0}[type=button]:-moz-focusring,button:-moz-focusring{outline:1px dotted ButtonText}fieldset{padding:.35em .75em .625em}legend{box-sizing:border-box;color:inherit;display:table;max-width:100%;padding:0;white-space:normal}progress{vertical-align:baseline}textarea{overflow:auto}[type=number]::-webkit-inner-spin-button,[type=number]::-webkit-outer-spin-button{height:auto}[type=search]{-webkit-appearance:textfield;outline-offset:-2px}[type=search]::-webkit-search-decoration{-webkit-appearance:none}details{display:block}summary{display:list-item}[hidden],template{display:none}blockquote,dd,dl,figure,h1,h2,h3,h4,h5,h6,hr,p,pre{margin:0}button{background-color:transparent;background-image:none}button:focus{outline:1px dotted;outline:5px auto -webkit-focus-ring-color}fieldset,ol,ul{margin:0;padding:0}ol,ul{list-style:none}html{font-family:system-ui,-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Arial,Noto Sans,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol,Noto Color Emoji;line-height:1.5}*,:after,:before{box-sizing:border-box;border:0 solid #e1e4e8}hr{border-top-width:1px}img{border-style:solid}textarea{resize:vertical}input::placeholder,textarea::placeholder{color:#a0aec0}button{cursor:pointer}table{border-collapse:collapse}h1,h2,h3,h4,h5,h6{font-size:inherit;font-weight:inherit}a{color:inherit;text-decoration:inherit}button,input,optgroup,select,textarea{padding:0;line-height:inherit;color:inherit}code,kbd,pre,samp{font-family:Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace}audio,canvas,embed,iframe,img,object,svg,video{display:block;vertical-align:middle}img,video{max-width:100%;height:auto}body{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Helvetica,Arial,sans-serif,Apple Color Emoji,Segoe UI Emoji;--text-opacity:1;color:#24292e;color:rgba(36,41,46,var(--text-opacity))}*{transition:width .2s ease-in}a{--text-opacity:1;color:#0366d6;color:rgba(3,102,214,var(--text-opacity))}a:hover{text-decoration:underline}.text-mono{font-family:SFMono-Regular,Consolas,Liberation Mono,Menlo,monospace!important}article{line-height:1.5;font-size:1rem;line-height:1.5rem;word-wrap:break-word;overflow-wrap:break-word}article>*{margin-bottom:1rem}article>h1,article>h2,article>h3,article>h4,article>h5,article>h6{font-weight:600;line-height:1.25;margin-top:1.5rem}article>:first-child{margin-top:0}article>h1{font-size:1.875rem;line-height:2.25rem}article>h1,article>h2{padding-bottom:.5rem;border-bottom-width:1px}article>h2{font-size:1.5rem;line-height:2rem}article>h3{font-size:1.25rem;line-height:1.75rem}article>h4{font-size:1.125rem;line-height:1.75rem}article>h5{font-size:1rem;line-height:1.5rem}article>h6{font-size:.875rem;line-height:1.25rem;--text-opacity:1;color:#6a737d;color:rgba(106,115,125,var(--text-opacity))}article a:link{--text-opacity:1;color:#0366d6;color:rgba(3,102,214,var(--text-opacity))}article ol,article ul{padding-left:1.5rem}article ul{list-style:disc}article ul ul{list-style:circle}article ul ul ul{list-style:square}article ol{list-style:decimal}article ol ol{list-style:lower-roman}article ol ol ol{list-style:lower-alpha}article hr{margin-top:1.5rem;margin-bottom:1.5rem;--bg-opacity:1;background-color:#e1e4e8;background-color:rgba(225,228,232,var(--bg-opacity));height:2px;content:" "}article img{max-width:100%}article>blockquote{padding-left:1rem;padding-right:1rem;--text-opacity:1;color:#6a737d;color:rgba(106,115,125,var(--text-opacity));border-left-width:4px}article pre{border-radius:.375rem;padding:1rem;font-size:.75rem;line-height:1rem;overflow:auto}article code{border-radius:.375rem;--bg-opacity:1;background-color:#f6f8fa;background-color:rgba(246,248,250,var(--bg-opacity));padding:1px .25rem;font-size:.875rem;line-height:1.25rem}article>table{display:block;width:100%;overflow:auto;border-spacing:0;border-collapse:collapse}article>table tr{border-width:1px;--border-opacity:1;border-color:#e1e4e8;border-color:rgba(225,228,232,var(--border-opacity))}article>table tr:nth-child(2n){--bg-opacity:1;background-color:#f6f8fa;background-color:rgba(246,248,250,var(--bg-opacity))}article>table td,article>table th{padding:.5rem 1rem;border-width:1px;--border-opacity:1;border-color:#e1e4e8;border-color:rgba(225,228,232,var(--border-opacity))}article>table th{font-weight:600}article>kbd{display:inline-block;padding:3px 5px;font-size:11px;line-height:10px;color:#444d56;vertical-align:middle;background-color:#fafbfc;border:1px solid #c6cbd1;border-bottom-color:#959da5;border-radius:3px;box-shadow:inset 0 -1px 0 #959da5}article .prism-code .token-line:last-child{height:0}._readme{line-height:1.5;font-size:1rem;line-height:1.5rem;word-wrap:break-word;overflow-wrap:break-word}._readme>*{margin-bottom:1rem}._readme>h1,._readme>h2,._readme>h3,._readme>h4,._readme>h5,._readme>h6{font-weight:600;line-height:1.25;margin-top:1.5rem}._readme>:first-child{margin-top:0}._readme>h1{font-size:1.875rem;line-height:2.25rem}._readme>h1,._readme>h2{padding-bottom:.5rem;border-bottom-width:1px}._readme>h2{font-size:1.5rem;line-height:2rem}._readme>h3{font-size:1.25rem;line-height:1.75rem}._readme>h4{font-size:1.125rem;line-height:1.75rem}._readme>h5{font-size:1rem;line-height:1.5rem}._readme>h6{font-size:.875rem;line-height:1.25rem;--text-opacity:1;color:#6a737d;color:rgba(106,115,125,var(--text-opacity))}._readme a:link{--text-opacity:1;color:#0366d6;color:rgba(3,102,214,var(--text-opacity))}._readme ol,._readme ul{padding-left:1.5rem}._readme ul{list-style:disc}._readme ul ul{list-style:circle}._readme ul ul ul{list-style:square}._readme ol{list-style:decimal}._readme ol ol{list-style:lower-roman}._readme ol ol ol{list-style:lower-alpha}._readme hr{margin-top:1.5rem;margin-bottom:1.5rem;--bg-opacity:1;background-color:#e1e4e8;background-color:rgba(225,228,232,var(--bg-opacity));height:2px;content:" "}._readme img{max-width:100%}._readme>blockquote{padding-left:1rem;padding-right:1rem;--text-opacity:1;color:#6a737d;color:rgba(106,115,125,var(--text-opacity));border-left-width:4px}._readme pre{border-radius:.375rem;padding:1rem;font-size:.75rem;line-height:1rem;overflow:auto}._readme code{border-radius:.375rem;--bg-opacity:1;background-color:#f6f8fa;background-color:rgba(246,248,250,var(--bg-opacity));padding:1px .25rem;font-size:.875rem;line-height:1.25rem}._readme>table{display:block;width:100%;overflow:auto;border-spacing:0;border-collapse:collapse}._readme>table tr{border-width:1px;--border-opacity:1;border-color:#e1e4e8;border-color:rgba(225,228,232,var(--border-opacity))}._readme>table tr:nth-child(2n){--bg-opacity:1;background-color:#f6f8fa;background-color:rgba(246,248,250,var(--bg-opacity))}._readme>table td,._readme>table th{padding:.5rem 1rem;border-width:1px;--border-opacity:1;border-color:#e1e4e8;border-color:rgba(225,228,232,var(--border-opacity))}._readme>table th{font-weight:600}._readme>kbd{display:inline-block;padding:3px 5px;font-size:11px;line-height:10px;color:#444d56;vertical-align:middle;background-color:#fafbfc;border:1px solid #c6cbd1;border-bottom-color:#959da5;border-radius:3px;box-shadow:inset 0 -1px 0 #959da5}._readme .prism-code .token-line:last-child{height:0}._readme p img{display:inline-block}.page-grid{width:100%;max-width:1280px;padding-left:1rem;padding-right:1rem;margin-left:auto;margin-right:auto;display:flex;flex-wrap:wrap}.topic-tag{--bg-opacity:1;background-color:#f1f8ff;background-color:rgba(241,248,255,var(--bg-opacity));border-radius:9999px;padding-left:.75rem;padding-right:.75rem;margin-bottom:.25rem;border-width:1px;--border-opacity:1;border-color:#fff;border-color:rgba(255,255,255,var(--border-opacity));font-size:.75rem;line-height:1rem;font-weight:500;line-height:1.5rem;display:inline-block}.topic-tag:hover{--bg-opacity:1;background-color:#def;background-color:rgba(221,238,255,var(--bg-opacity));text-decoration:none}.calendar-legend{display:inline-flex;justify-content:space-between;align-items:center;margin-left:.5rem;margin-right:.5rem;width:4rem}.calendar-legend li{content:" ";width:10px;height:10px;box-shadow:inset 0 0 0 1px rgba(27,31,35,.04);border-radius:.125rem;display:inline-block}.bg-white{--bg-opacity:1;background-color:#fff;background-color:rgba(255,255,255,var(--bg-opacity))}.bg-gray-100{--bg-opacity:1;background-color:#f6f8fa;background-color:rgba(246,248,250,var(--bg-opacity))}.bg-gray-120{--bg-opacity:1;background-color:#ebedf0;background-color:rgba(235,237,240,var(--bg-opacity))}.bg-gray-dark{--bg-opacity:1;background-color:#24292e;background-color:rgba(36,41,46,var(--bg-opacity))}.bg-yellow-light{--bg-opacity:1;background-color:#fff5b1;background-color:rgba(255,245,177,var(--bg-opacity))}.bg-green-superlight{--bg-opacity:1;background-color:#9be9a8;background-color:rgba(155,233,168,var(--bg-opacity))}.bg-green-light{--bg-opacity:1;background-color:#40c463;background-color:rgba(64,196,99,var(--bg-opacity))}.bg-green-medium{--bg-opacity:1;background-color:#30a14e;background-color:rgba(48,161,78,var(--bg-opacity))}.bg-green-dark{--bg-opacity:1;background-color:#216e39;background-color:rgba(33,110,57,var(--bg-opacity))}.bg-blue-100{--bg-opacity:1;background-color:#f1f8ff;background-color:rgba(241,248,255,var(--bg-opacity))}.bg-blue{--bg-opacity:1;background-color:#0366d6;background-color:rgba(3,102,214,var(--bg-opacity))}.hover\:bg-gray-100:hover{--bg-opacity:1;background-color:#f6f8fa;background-color:rgba(246,248,250,var(--bg-opacity))}.hover\:bg-blue:hover{--bg-opacity:1;background-color:#0366d6;background-color:rgba(3,102,214,var(--bg-opacity))}.border-collapse{border-collapse:collapse}.border-yellow{--border-opacity:1;border-color:#f9c513;border-color:rgba(249,197,19,var(--border-opacity))}.border-blue-light{--border-opacity:1;border-color:#c8e1ff;border-color:rgba(200,225,255,var(--border-opacity))}.rounded-md{border-radius:.375rem}.rounded-full{border-radius:9999px}.rounded-t-md{border-top-left-radius:.375rem;border-top-right-radius:.375rem}.border{border-width:1px}.border-t{border-top-width:1px}.border-b{border-bottom-width:1px}.cursor-pointer{cursor:pointer}.block{display:block}.inline-block{display:inline-block}.inline{display:inline}.flex{display:flex}.inline-flex{display:inline-flex}.table{display:table}.grid{display:grid}.hidden{display:none}.flex-row{flex-direction:row}.flex-col{flex-direction:column}.flex-wrap{flex-wrap:wrap}.items-end{align-items:flex-end}.items-center{align-items:center}.self-end{align-self:flex-end}.justify-end{justify-content:flex-end}.justify-between{justify-content:space-between}.flex-grow{flex-grow:1}.order-1{order:1}.font-light{font-weight:300}.font-medium{font-weight:500}.font-semibold{font-weight:600}.h-0{height:0}.h-12{height:3rem}.h-auto{height:auto}.text-xs{font-size:.75rem;line-height:1rem}.text-sm{font-size:.875rem;line-height:1.25rem}.text-base{font-size:1rem;line-height:1.5rem}.text-lg{font-size:1.125rem}.text-lg,.text-xl{line-height:1.75rem}.text-xl{font-size:1.25rem}.text-2xl{font-size:1.5rem;line-height:2rem}.text-3xl{font-size:1.875rem;line-height:2.25rem}.text-11px{font-size:11px}.text-26px{font-size:26px}.leading-6{line-height:1.5rem}.leading-tight{line-height:1.25}.list-circle{list-style-type:circle}.my-2{margin-top:.5rem;margin-bottom:.5rem}.mx-2{margin-left:.5rem;margin-right:.5rem}.my-4{margin-top:1rem;margin-bottom:1rem}.-mx-2{margin-left:-.5rem;margin-right:-.5rem}.-mx-4{margin-left:-1rem;margin-right:-1rem}.mr-0{margin-right:0}.mb-0{margin-bottom:0}.mt-1{margin-top:.25rem}.mr-1{margin-right:.25rem}.mb-1{margin-bottom:.25rem}.ml-1{margin-left:.25rem}.mt-2{margin-top:.5rem}.mr-2{margin-right:.5rem}.mb-2{margin-bottom:.5rem}.ml-2{margin-left:.5rem}.ml-3{margin-left:.75rem}.mt-4{margin-top:1rem}.mr-4{margin-right:1rem}.mb-4{margin-bottom:1rem}.ml-5{margin-left:1.25rem}.mt-6{margin-top:1.5rem}.mb-6{margin-bottom:1.5rem}.mt-8{margin-top:2rem}.mb-8{margin-bottom:2rem}.mt-10{margin-top:2.5rem}.mt-3px{margin-top:3px}.-mr-2{margin-right:-.5rem}.min-h-screen{min-height:100vh}.opacity-50{opacity:.5}.opacity-75{opacity:.75}.overflow-hidden{overflow:hidden}.overflow-x-scroll{overflow-x:scroll}.p-2{padding:.5rem}.p-4{padding:1rem}.p-6{padding:1.5rem}.py-1{padding-top:.25rem;padding-bottom:.25rem}.px-1{padding-left:.25rem;padding-right:.25rem}.py-2{padding-top:.5rem;padding-bottom:.5rem}.px-2{padding-left:.5rem;padding-right:.5rem}.px-3{padding-left:.75rem;padding-right:.75rem}.py-4{padding-top:1rem;padding-bottom:1rem}.px-4{padding-left:1rem;padding-right:1rem}.py-6{padding-top:1.5rem;padding-bottom:1.5rem}.py-px{padding-top:1px;padding-bottom:1px}.px-2px{padding-left:2px;padding-right:2px}.pt-1{padding-top:.25rem}.pl-1{padding-left:.25rem}.pb-2{padding-bottom:.5rem}.pb-4{padding-bottom:1rem}.pt-6{padding-top:1.5rem}.pl-8{padding-left:2rem}.pt-10{padding-top:2.5rem}.absolute{position:absolute}.relative{position:relative}.sticky{position:sticky}.top-0{top:0}.left-1{left:.25rem}.-top-2px{top:-2px}.left-10px{left:10px}.top-74px{top:74px}.text-white{--text-opacity:1;color:#fff;color:rgba(255,255,255,var(--text-opacity))}.text-gray-200{--text-opacity:1;color:#959da5;color:rgba(149,157,165,var(--text-opacity))}.text-gray-light{--text-opacity:1;color:#e1e4e8;color:rgba(225,228,232,var(--text-opacity))}.text-gray-medium{--text-opacity:1;color:#767676;color:rgba(118,118,118,var(--text-opacity))}.text-gray-main{--text-opacity:1;color:#586069;color:rgba(88,96,105,var(--text-opacity))}.text-gray-dark{--text-opacity:1;color:#24292e;color:rgba(36,41,46,var(--text-opacity))}.hover\:text-blue:hover,.text-blue{--text-opacity:1;color:#0366d6;color:rgba(3,102,214,var(--text-opacity))}.hover\:text-opacity-75:hover{--text-opacity:0.75}.hover\:no-underline:hover,.no-underline{text-decoration:none}.whitespace-no-wrap{white-space:nowrap}.w-24{width:6rem}.w-auto{width:auto}.w-2\/12{width:16.666667%}.w-10\/12{width:83.333333%}.w-full{width:100%}.w-screen{width:100vw}.z-10{z-index:10}.gap-4{grid-gap:1rem;gap:1rem}.grid-cols-1{grid-template-columns:repeat(1,minmax(0,1fr))}.transform{--transform-translate-x:0;--transform-translate-y:0;--transform-rotate:0;--transform-skew-x:0;--transform-skew-y:0;--transform-scale-x:1;--transform-scale-y:1;transform:translateX(var(--transform-translate-x)) translateY(var(--transform-translate-y)) rotate(var(--transform-rotate)) skewX(var(--transform-skew-x)) skewY(var(--transform-skew-y)) scaleX(var(--transform-scale-x)) scaleY(var(--transform-scale-y))}.transition{transition-property:background-color,border-color,color,fill,stroke,opacity,box-shadow,transform}@keyframes spin{to{transform:rotate(1turn)}}@keyframes ping{75%,to{transform:scale(2);opacity:0}}@keyframes pulse{50%{opacity:.5}}@keyframes bounce{0%,to{transform:translateY(-25%);animation-timing-function:cubic-bezier(.8,0,1,1)}50%{transform:none;animation-timing-function:cubic-bezier(0,0,.2,1)}}@media (min-width:768px){.md\:rounded-b-md{border-bottom-right-radius:.375rem;border-bottom-left-radius:.375rem}.md\:border{border-width:1px}.md\:border-b-0{border-bottom-width:0}.md\:border-l{border-left-width:1px}.md\:block{display:block}.md\:flex{display:flex}.md\:hidden{display:none}.md\:flex-row{flex-direction:row}.md\:items-center{align-items:center}.md\:justify-start{justify-content:flex-start}.md\:justify-between{justify-content:space-between}.md\:order-1{order:1}.md\:h-60px{height:60px}.md\:text-xl{font-size:1.25rem;line-height:1.75rem}.md\:text-3xl{font-size:1.875rem;line-height:2.25rem}.md\:mx-0{margin-left:0;margin-right:0}.md\:mx-4{margin-left:1rem;margin-right:1rem}.md\:-mx-4{margin-left:-1rem;margin-right:-1rem}.md\:mr-0{margin-right:0}.md\:mb-0{margin-bottom:0}.md\:mr-2{margin-right:.5rem}.md\:mb-4{margin-bottom:1rem}.md\:mb-10{margin-bottom:2.5rem}.md\:ml-14{margin-left:3.5rem}.md\:-mt-8{margin-top:-2rem}.md\:overflow-hidden{overflow:hidden}.md\:py-0{padding-top:0;padding-bottom:0}.md\:py-4{padding-top:1rem;padding-bottom:1rem}.md\:px-4{padding-left:1rem;padding-right:1rem}.md\:px-6{padding-left:1.5rem;padding-right:1.5rem}.md\:px-8{padding-left:2rem;padding-right:2rem}.md\:pr-4{padding-right:1rem}.md\:pl-4{padding-left:1rem}.md\:-top-4px{top:-4px}.md\:left-8px{left:8px}.md\:w-1\/2{width:50%}.md\:w-2\/12{width:16.666667%}.md\:w-3\/12{width:25%}.md\:w-5\/12{width:41.666667%}.md\:w-9\/12{width:75%}.md\:w-10\/12{width:83.333333%}.md\:w-full{width:100%}.md\:z-20{z-index:20}.md\:grid-cols-2{grid-template-columns:repeat(2,minmax(0,1fr))}}@media (min-width:1280px){.xl\:items-center{align-items:center}}</style><meta name="generator" content="Gatsby 2.32.13"/><link rel="preconnect dns-prefetch" href="https://www.google-analytics.com"/><title data-react-helmet="true">Nvidia 4070Ti cuda report - Abel Lee</title><link data-react-helmet="true" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg"/><link data-react-helmet="true" rel="canonical" href="https://www.abellee.cn/2023/11/04/nvidia_4070TI_cuda_report/"/><meta data-react-helmet="true" name="author" content="abelleeye"/><meta data-react-helmet="true" name="description" content="asyncAPI bandwidthTest batchCUBLAS bf16TensorCoreGemm binaryPartitionCG binomialOptions…"/><meta data-react-helmet="true" name="keywords" content="Nvidia"/><link rel="sitemap" type="application/xml" href="/sitemap.xml"/><style data-styled="" data-styled-version="5.3.11">.jgPWsE{color:#c6cbd1;}/*!sc*/
.jgPWsE:hover{color:#959da5;}/*!sc*/
data-styled.g5[id="sc-49bqba-0"]{content:"jgPWsE,"}/*!sc*/
.dDfbur{position:absolute;z-index:2;width:14px;height:14px;color:var(--color-text-inverse);background-image:linear-gradient(#54a3ff,#006eed);background-clip:padding-box;border:2px solid #24292e;border-radius:50%;}/*!sc*/
data-styled.g7[id="sc-1n1400d-1"]{content:"dDfbur,"}/*!sc*/
.hcGhuZ:before{content:' ';width:10px;height:10px;position:absolute;border-left:1px solid #c8e1ff;border-bottom:1px solid #c8e1ff;background:#f1f8ff;left:-5px;-webkit-transform:rotate(45deg);-ms-transform:rotate(45deg);transform:rotate(45deg);}/*!sc*/
data-styled.g11[id="sc-gf6yrx-0"]{content:"hcGhuZ,"}/*!sc*/
</style><link as="script" rel="preload" href="/webpack-runtime-c8d2f823f313dc640930.js"/><link as="script" rel="preload" href="/framework-303b5b995007c0324953.js"/><link as="script" rel="preload" href="/styles-4c7d46604086483aa196.js"/><link as="script" rel="preload" href="/app-c2e304ea278d1bbd77b8.js"/><link as="script" rel="preload" href="/abba1267-4db10b0857c7c5274881.js"/><link as="script" rel="preload" href="/fa5f8105-cb81053d7202d517e9c0.js"/><link as="script" rel="preload" href="/e5908a55-3d0292d81ee122ce0f53.js"/><link as="script" rel="preload" href="/a490077e-d0b9734bbaaad323dc86.js"/><link as="script" rel="preload" href="/aa642b0a-84f2665e5b07251329ab.js"/><link as="script" rel="preload" href="/e2070dfd-b06a1c5dc488eaac3595.js"/><link as="script" rel="preload" href="/b3ead0f8-e70b62a53986d7cc5d98.js"/><link as="script" rel="preload" href="/43ccf438-9f3768ab58fc48b692cf.js"/><link as="script" rel="preload" href="/aacb0a1bb7a50c0bda0baac92d46406327a248f1-1bd85d960f2e5a815fd6.js"/><link as="script" rel="preload" href="/component---gatsby-theme-replica-src-templates-post-tsx-010a4a90954a64823958.js"/><link as="fetch" rel="preload" href="/page-data/2023/11/04/nvidia_4070TI_cuda_report/page-data.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/sq/d/2873555300.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/sq/d/3400548236.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/sq/d/822196256.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/app-data.json" crossorigin="anonymous"/></head><body><div id="___gatsby"><div style="outline:none" tabindex="-1" id="gatsby-focus-wrapper"><div class="flex flex-col w-full min-h-screen"><div class="bg-gray-dark p-4 md:py-0 md:px-6"><div class="w-full flex justify-between md:justify-auto items-center md:h-60px"><div class="block md:hidden text-white text-3xl"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M904 160H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0 624H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0-312H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8z"></path></svg></div><a class="text-white" href="/"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" size="32" height="32" width="32" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a><div class="hidden md:flex items-center flex-grow px-4"><div class="flex ml-2 text-sm font-semibold"><a class="mx-2 py-4 text-white hover:no-underline hover:text-opacity-75" href="/"><span>Overview</span></a><a class="mx-2 py-4 text-white hover:no-underline hover:text-opacity-75" href="/archives"><span>Archives</span></a><a class="mx-2 py-4 text-white hover:no-underline hover:text-opacity-75" href="/categories"><span>Categories</span></a><a class="mx-2 py-4 text-white hover:no-underline hover:text-opacity-75" href="/tags"><span>Tags</span></a><a href="/atom.xml" class="mx-2 py-4 text-white hover:no-underline hover:text-opacity-75" target="_blank">RSS</a></div></div><ul class="flex items-center text-white font-semibold"><li class="relative"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" class="mr-0 md:mr-2 text-2xl md:text-xl" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M289.7 403c-6.1 0-11.4 4.2-12.7 10.2-1 4.5-2.7 8.2-5 10.9-1.3 1.5-5.1 5.9-16.1 5.9-11 0-14.8-4.5-16.1-5.9-2.3-2.7-4-6.4-5-10.9-1.3-6-6.6-10.2-12.7-10.2-8.4 0-14.5 7.8-12.7 15.9 5 22.3 21 37.1 46.5 37.1s41.5-14.7 46.5-37.1c1.8-8.1-4.4-15.9-12.7-15.9zM412 352.2c-15.4-20.3-45.7-32.2-45.7-123.1 0-93.3-41.2-130.8-79.6-139.8-3.6-.9-6.2-2.1-6.2-5.9v-2.9c0-13.3-10.8-24.6-24-24.6h-.6c-13.2 0-24 11.3-24 24.6v2.9c0 3.7-2.6 5-6.2 5.9-38.5 9.1-79.6 46.5-79.6 139.8 0 90.9-30.3 102.7-45.7 123.1-9.9 13.1-.5 31.8 15.9 31.8h280.1c16.1 0 25.4-18.8 15.6-31.8zm-39 5.8H139.8c-3.8 0-5.8-4.4-3.3-7.3 7-8 14.7-18.5 21-33.4 9.6-22.6 14.3-51.5 14.3-88.2 0-37.3 7-66.5 20.9-86.8 12.4-18.2 27.9-25.1 38.7-27.6 8.4-2 14.4-5.8 18.6-10.5 3.2-3.6 8.7-3.8 11.9-.2 5.1 5.7 12 9.1 18.8 10.7 10.8 2.5 26.3 9.4 38.7 27.6 13.9 20.3 20.9 49.5 20.9 86.8 0 36.7 4.7 65.6 14.3 88.2 6.5 15.2 14.4 25.9 21.5 33.9 2.2 2.7.4 6.8-3.1 6.8z"></path></svg><div class="sc-1n1400d-1 dDfbur -top-2px left-10px md:left-8px md:-top-4px"></div></li><li class="hidden md:flex mr-2"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" class="text-xl relative left-1" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M368.5 240H272v-96.5c0-8.8-7.2-16-16-16s-16 7.2-16 16V240h-96.5c-8.8 0-16 7.2-16 16 0 4.4 1.8 8.4 4.7 11.3 2.9 2.9 6.9 4.7 11.3 4.7H240v96.5c0 4.4 1.8 8.4 4.7 11.3 2.9 2.9 6.9 4.7 11.3 4.7 8.8 0 16-7.2 16-16V272h96.5c8.8 0 16-7.2 16-16s-7.2-16-16-16z"></path></svg><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 24 24" class="text-xl" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><g><path fill="none" d="M0 0h24v24H0z"></path><path d="M12 14l-4-4h8z"></path></g></svg></li><li class="hidden md:flex -mr-2"><img src="https://s2.loli.net/2023/10/24/j6AUMN7rTFy9zE4.jpg" class="rounded-full" width="20"/><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 24 24" class="text-xl" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><g><path fill="none" d="M0 0h24v24H0z"></path><path d="M12 14l-4-4h8z"></path></g></svg></li></ul></div><div class="h-0 w-full font-semibold overflow-hidden"><a class="py-2 text-white hover:no-underline block border-t-fade-white15" href="/"><span>Overview</span></a><a class="py-2 text-white hover:no-underline block border-t-fade-white15" href="/archives"><span>Archives</span></a><a class="py-2 text-white hover:no-underline block border-t-fade-white15" href="/categories"><span>Categories</span></a><a class="py-2 text-white hover:no-underline block border-t-fade-white15" href="/tags"><span>Tags</span></a><a href="/atom.xml" class="py-2 text-white hover:no-underline block border-t-fade-white15" target="_blank">RSS</a><a class="py-2 text-white hover:no-underline flex md:hidden items-center border-t-fade-white15" href="/"><img src="https://s2.loli.net/2023/10/24/j6AUMN7rTFy9zE4.jpg" class="rounded-full mr-2" width="20"/>abelleeye</a></div></div><div class="page-grid mt-4 md:px-8"><div class="w-full mb-4"><h1 class="text-2xl md:text-3xl mb-2">Nvidia 4070Ti cuda report<!-- --> <span class="text-gray-200">#<!-- -->33</span></h1><div class="text-sm text-gray-main border-b md:mb-4 pb-4">Posted <!-- -->Just now<!-- --> · <!-- -->1<!-- --> mins reading</div></div><div class="w-full md:w-9/12 md:pr-4 md:border-b-0 mb-4"><div class="relative"><img src="https://s2.loli.net/2023/10/24/j6AUMN7rTFy9zE4.jpg" class="hidden md:block border rounded-full absolute" width="40"/><div class="md:ml-14"><div class="sc-gf6yrx-0 hcGhuZ hidden md:flex items-center justify-between text-xs relative bg-blue-100 text-gray-main
      border md:border-b-0 rounded-t-md border-blue-light py-2 px-4"><div class="leading-6"><img src="https://s2.loli.net/2023/10/24/j6AUMN7rTFy9zE4.jpg" class="inline md:hidden rounded-full mr-2" width="24"/><strong>abelleeye</strong> posted on<!-- --> <time dateTime="2023-11-04T16:04:00.000Z" title="2023-11-04T16:04:00.000Z">Nov 4</time></div><div class="flex items-center"><span class="border border-blue-light rounded-full px-2 py-px hidden md:block"><strong>Author</strong></span><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" size="18" class="hidden md:block md:mx-4" height="18" width="18" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.111 2.18a7 7 0 1 1 7.778 11.64A7 7 0 0 1 4.11 2.18zm.556 10.809a6 6 0 1 0 6.666-9.978 6 6 0 0 0-6.666 9.978zM6.5 7a1 1 0 1 1-2 0 1 1 0 0 1 2 0zm5 0a1 1 0 1 1-2 0 1 1 0 0 1 2 0zM8 11a3 3 0 0 1-2.65-1.58l-.87.48a4 4 0 0 0 7.12-.16l-.9-.43A3 3 0 0 1 8 11z"></path></svg><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" size="18" height="18" width="18" xmlns="http://www.w3.org/2000/svg"><path d="M4 8a1 1 0 1 1-2 0 1 1 0 0 1 2 0zm5 0a1 1 0 1 1-2 0 1 1 0 0 1 2 0zm5 0a1 1 0 1 1-2 0 1 1 0 0 1 2 0z"></path></svg></div></div><article class="border-b md:border md:rounded-b-md md:px-8 md:py-4"><h1 id="-asyncapi">asyncAPI</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">[./asyncAPI] - Starting...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">GPU Device 0: &quot;Ada&quot; with compute capability 8.9</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">CUDA device [NVIDIA GeForce RTX 4070 Ti]</span></div><div class="token-line" style="color:#393A34"><span class="token plain">time spent executing by the GPU: 5.63</span></div><div class="token-line" style="color:#393A34"><span class="token plain">time spent by CPU in CUDA calls: 2.81</span></div><div class="token-line" style="color:#393A34"><span class="token plain">CPU executed 48686 iterations while waiting for GPU to finish</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-bandwidthtest">bandwidthTest</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">[CUDA Bandwidth Test] - Starting...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Running on...</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain"> Device 0: NVIDIA GeForce RTX 4070 Ti</span></div><div class="token-line" style="color:#393A34"><span class="token plain"> Quick Mode</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain"> Host to Device Bandwidth, 1 Device(s)</span></div><div class="token-line" style="color:#393A34"><span class="token plain"> PINNED Memory Transfers</span></div><div class="token-line" style="color:#393A34"><span class="token plain">   Transfer Size (Bytes)    Bandwidth(GB/s)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">   32000000         24.0</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain"> Device to Host Bandwidth, 1 Device(s)</span></div><div class="token-line" style="color:#393A34"><span class="token plain"> PINNED Memory Transfers</span></div><div class="token-line" style="color:#393A34"><span class="token plain">   Transfer Size (Bytes)    Bandwidth(GB/s)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">   32000000         26.3</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain"> Device to Device Bandwidth, 1 Device(s)</span></div><div class="token-line" style="color:#393A34"><span class="token plain"> PINNED Memory Transfers</span></div><div class="token-line" style="color:#393A34"><span class="token plain">   Transfer Size (Bytes)    Bandwidth(GB/s)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">   32000000         457.7</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Result = PASS</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-batchcublas">batchCUBLAS</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">batchCUBLAS Starting...</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">GPU Device 0: &quot;Ada&quot; with compute capability 8.9</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain"> ==== Running single kernels ==== </span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Testing sgemm</span></div><div class="token-line" style="color:#393A34"><span class="token plain">#### args: ta=0 tb=0 m=128 n=128 k=128  alpha = (0xbf800000, -1) beta= (0x40000000, 2)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">#### args: lda=128 ldb=128 ldc=128</span></div><div class="token-line" style="color:#393A34"><span class="token plain">^^^^ elapsed = 0.00437307 sec  GFLOPS=0.95912</span></div><div class="token-line" style="color:#393A34"><span class="token plain">@@@@ sgemm test OK</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Testing dgemm</span></div><div class="token-line" style="color:#393A34"><span class="token plain">#### args: ta=0 tb=0 m=128 n=128 k=128  alpha = (0x0000000000000000, 0) beta= (0x0000000000000000, 0)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">#### args: lda=128 ldb=128 ldc=128</span></div><div class="token-line" style="color:#393A34"><span class="token plain">^^^^ elapsed = 0.00002599 sec  GFLOPS=161.396</span></div><div class="token-line" style="color:#393A34"><span class="token plain">@@@@ dgemm test OK</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain"> ==== Running N=10 without streams ==== </span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Testing sgemm</span></div><div class="token-line" style="color:#393A34"><span class="token plain">#### args: ta=0 tb=0 m=128 n=128 k=128  alpha = (0xbf800000, -1) beta= (0x00000000, 0)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">#### args: lda=128 ldb=128 ldc=128</span></div><div class="token-line" style="color:#393A34"><span class="token plain">^^^^ elapsed = 0.00005698 sec  GFLOPS=736.075</span></div><div class="token-line" style="color:#393A34"><span class="token plain">@@@@ sgemm test OK</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Testing dgemm</span></div><div class="token-line" style="color:#393A34"><span class="token plain">#### args: ta=0 tb=0 m=128 n=128 k=128  alpha = (0xbff0000000000000, -1) beta= (0x0000000000000000, 0)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">#### args: lda=128 ldb=128 ldc=128</span></div><div class="token-line" style="color:#393A34"><span class="token plain">^^^^ elapsed = 0.00040507 sec  GFLOPS=103.544</span></div><div class="token-line" style="color:#393A34"><span class="token plain">@@@@ dgemm test OK</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain"> ==== Running N=10 with streams ==== </span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Testing sgemm</span></div><div class="token-line" style="color:#393A34"><span class="token plain">#### args: ta=0 tb=0 m=128 n=128 k=128  alpha = (0x40000000, 2) beta= (0x40000000, 2)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">#### args: lda=128 ldb=128 ldc=128</span></div><div class="token-line" style="color:#393A34"><span class="token plain">^^^^ elapsed = 0.00006294 sec  GFLOPS=666.371</span></div><div class="token-line" style="color:#393A34"><span class="token plain">@@@@ sgemm test OK</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Testing dgemm</span></div><div class="token-line" style="color:#393A34"><span class="token plain">#### args: ta=0 tb=0 m=128 n=128 k=128  alpha = (0xbff0000000000000, -1) beta= (0x0000000000000000, 0)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">#### args: lda=128 ldb=128 ldc=128</span></div><div class="token-line" style="color:#393A34"><span class="token plain">^^^^ elapsed = 0.00009298 sec  GFLOPS=451.082</span></div><div class="token-line" style="color:#393A34"><span class="token plain">@@@@ dgemm test OK</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain"> ==== Running N=10 batched ==== </span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Testing sgemm</span></div><div class="token-line" style="color:#393A34"><span class="token plain">#### args: ta=0 tb=0 m=128 n=128 k=128  alpha = (0x3f800000, 1) beta= (0xbf800000, -1)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">#### args: lda=128 ldb=128 ldc=128</span></div><div class="token-line" style="color:#393A34"><span class="token plain">^^^^ elapsed = 0.00007200 sec  GFLOPS=582.523</span></div><div class="token-line" style="color:#393A34"><span class="token plain">@@@@ sgemm test OK</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Testing dgemm</span></div><div class="token-line" style="color:#393A34"><span class="token plain">#### args: ta=0 tb=0 m=128 n=128 k=128  alpha = (0xbff0000000000000, -1) beta= (0x4000000000000000, 2)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">#### args: lda=128 ldb=128 ldc=128</span></div><div class="token-line" style="color:#393A34"><span class="token plain">^^^^ elapsed = 0.00043511 sec  GFLOPS=96.3955</span></div><div class="token-line" style="color:#393A34"><span class="token plain">@@@@ dgemm test OK</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Test Summary</span></div><div class="token-line" style="color:#393A34"><span class="token plain">0 error(s)</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-bf16tensorcoregemm">bf16TensorCoreGemm</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">Initializing...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">GPU Device 0: &quot;Ada&quot; with compute capability 8.9</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">M: 8192 (16 x 512)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">N: 8192 (16 x 512)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">K: 8192 (16 x 512)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Preparing data for GPU...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Required shared memory size: 72 Kb</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Computing using high performance kernel = 0 - compute_bf16gemm_async_copy</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Time: 20.165665 ms</span></div><div class="token-line" style="color:#393A34"><span class="token plain">TFLOPS: 54.52</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-binarypartitioncg">binaryPartitionCG</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">GPU Device 0: &quot;Ada&quot; with compute capability 8.9</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Launching 120 blocks with 768 threads...</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Array size = 102400 Num of Odds = 50945 Sum of Odds = 1272565 Sum of Evens 1233938</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">...Done.</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-binomialoptions">binomialOptions</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">[./binomialOptions] - Starting...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">GPU Device 0: &quot;Ada&quot; with compute capability 8.9</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Generating input data...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Running GPU binomial tree...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Options count            : 1024     </span></div><div class="token-line" style="color:#393A34"><span class="token plain">Time steps               : 2048     </span></div><div class="token-line" style="color:#393A34"><span class="token plain">binomialOptionsGPU() time: 0.640000 msec</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Options per second       : 1600000.035763     </span></div><div class="token-line" style="color:#393A34"><span class="token plain">Running CPU binomial tree...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Comparing the results...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">GPU binomial vs. Black-Scholes</span></div><div class="token-line" style="color:#393A34"><span class="token plain">L1 norm: 2.220214E-04</span></div><div class="token-line" style="color:#393A34"><span class="token plain">CPU binomial vs. Black-Scholes</span></div><div class="token-line" style="color:#393A34"><span class="token plain">L1 norm: 2.220922E-04</span></div><div class="token-line" style="color:#393A34"><span class="token plain">CPU binomial vs. GPU binomial</span></div><div class="token-line" style="color:#393A34"><span class="token plain">L1 norm: 7.997008E-07</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Shutting down...</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Test passed</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-binomialoptions_nvrtc">binomialOptions_nvrtc</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">[./binomialOptions_nvrtc] - Starting...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Generating input data...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Running GPU binomial tree...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">&gt; Using CUDA Device [0]: NVIDIA GeForce RTX 4070 Ti</span></div><div class="token-line" style="color:#393A34"><span class="token plain">&gt; Using CUDA Device [0]: NVIDIA GeForce RTX 4070 Ti</span></div><div class="token-line" style="color:#393A34"><span class="token plain">&gt; GPU Device has SM 8.9 compute capability</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Options count            : 1024     </span></div><div class="token-line" style="color:#393A34"><span class="token plain">Time steps               : 2048     </span></div><div class="token-line" style="color:#393A34"><span class="token plain">binomialOptionsGPU() time: 142.667007 msec</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Options per second       : 7177.552949     </span></div><div class="token-line" style="color:#393A34"><span class="token plain">Running CPU binomial tree...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Comparing the results...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">GPU binomial vs. Black-Scholes</span></div><div class="token-line" style="color:#393A34"><span class="token plain">L1 norm: 2.216577E-04</span></div><div class="token-line" style="color:#393A34"><span class="token plain">CPU binomial vs. Black-Scholes</span></div><div class="token-line" style="color:#393A34"><span class="token plain">L1 norm: 9.435265E-05</span></div><div class="token-line" style="color:#393A34"><span class="token plain">CPU binomial vs. GPU binomial</span></div><div class="token-line" style="color:#393A34"><span class="token plain">L1 norm: 1.513570E-04</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Shutting down...</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Test passed</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-blackscholes">BlackScholes</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">[./BlackScholes] - Starting...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">GPU Device 0: &quot;Ada&quot; with compute capability 8.9</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Initializing data...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">...allocating CPU memory for options.</span></div><div class="token-line" style="color:#393A34"><span class="token plain">...allocating GPU memory for options.</span></div><div class="token-line" style="color:#393A34"><span class="token plain">...generating input data in CPU mem.</span></div><div class="token-line" style="color:#393A34"><span class="token plain">...copying input data to GPU mem.</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Data init done.</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Executing Black-Scholes GPU kernel (512 iterations)...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Options count             : 8000000     </span></div><div class="token-line" style="color:#393A34"><span class="token plain">BlackScholesGPU() time    : 0.180459 msec</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Effective memory bandwidth: 443.314048 GB/s</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Gigaoptions per second    : 44.331405     </span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">BlackScholes, Throughput = 44.3314 GOptions/s, Time = 0.00018 s, Size = 8000000 options, NumDevsUsed = 1, Workgroup = 128</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Reading back GPU results...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Checking the results...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">...running CPU calculations.</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Comparing the results...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">L1 norm: 1.741792E-07</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Max absolute error: 1.192093E-05</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Shutting down...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">...releasing GPU memory.</span></div><div class="token-line" style="color:#393A34"><span class="token plain">...releasing CPU memory.</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Shutdown done.</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">[BlackScholes] - Test Summary</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Test passed</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-blackscholes_nvrtc">BlackScholes_nvrtc</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">[./BlackScholes_nvrtc] - Starting...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Initializing data...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">...allocating CPU memory for options.</span></div><div class="token-line" style="color:#393A34"><span class="token plain">&gt; Using CUDA Device [0]: NVIDIA GeForce RTX 4070 Ti</span></div><div class="token-line" style="color:#393A34"><span class="token plain">&gt; Using CUDA Device [0]: NVIDIA GeForce RTX 4070 Ti</span></div><div class="token-line" style="color:#393A34"><span class="token plain">&gt; GPU Device has SM 8.9 compute capability</span></div><div class="token-line" style="color:#393A34"><span class="token plain">...allocating GPU memory for options.</span></div><div class="token-line" style="color:#393A34"><span class="token plain">...generating input data in CPU mem.</span></div><div class="token-line" style="color:#393A34"><span class="token plain">...copying input data to GPU mem.</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Data init done.</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Executing Black-Scholes GPU kernel (512 iterations)...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Options count             : 8000000     </span></div><div class="token-line" style="color:#393A34"><span class="token plain">BlackScholesGPU() time    : 0.180770 msec</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Effective memory bandwidth: 442.552452 GB/s</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Gigaoptions per second    : 44.255245     </span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">BlackScholes, Throughput = 44.2552 GOptions/s, Time = 0.00018 s, Size = 8000000 options, NumDevsUsed = 1, Workgroup = 128</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Reading back GPU results...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Checking the results...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">...running CPU calculations.</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Comparing the results...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">L1 norm: 1.741792E-07</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Max absolute error: 1.192093E-05</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Shutting down...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">...releasing GPU memory.</span></div><div class="token-line" style="color:#393A34"><span class="token plain">...releasing CPU memory.</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Shutdown done.</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">[./BlackScholes_nvrtc] - Test Summary</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Test passed</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-c11_cuda">c++11_cuda</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">GPU Device 0: &quot;Ada&quot; with compute capability 8.9</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Read 3223503 byte corpus from ../../../../Samples/0_Introduction/c++11_cuda/warandpeace.txt</span></div><div class="token-line" style="color:#393A34"><span class="token plain">counted 107310 instances of &#x27;x&#x27;, &#x27;y&#x27;, &#x27;z&#x27;, or &#x27;w&#x27; in &quot;../../../../Samples/0_Introduction/c++11_cuda/warandpeace.txt&quot;</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-cdpadvancedquicksort">cdpAdvancedQuicksort</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">GPU Device 0: &quot;Ada&quot; with compute capability 8.9</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">GPU device NVIDIA GeForce RTX 4070 Ti has compute capabilities (SM 8.9)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Running qsort on 1000000 elements with seed 0, on NVIDIA GeForce RTX 4070 Ti</span></div><div class="token-line" style="color:#393A34"><span class="token plain">    cdpAdvancedQuicksort PASSED</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Sorted 1000000 elems in 5.871 ms (170.341 Melems/sec)</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-cdpbeziertessellation">cdpBezierTessellation</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">Running on GPU 0 (NVIDIA GeForce RTX 4070 Ti)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Computing Bezier Lines (CUDA Dynamic Parallelism Version) ... Done!</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-cdpquadtree">cdpQuadtree</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">GPU Device 0: &quot;Ada&quot; with compute capability 8.9</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">GPU device NVIDIA GeForce RTX 4070 Ti has compute capabilities (SM 8.9)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Launching CDP kernel to build the quadtree</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Results: OK</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-cdpsimpleprint">cdpSimplePrint</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">starting Simple Print (CUDA Dynamic Parallelism)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">GPU Device 0: &quot;Ada&quot; with compute capability 8.9</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">***************************************************************************</span></div><div class="token-line" style="color:#393A34"><span class="token plain">The CPU launches 2 blocks of 2 threads each. On the device each thread will</span></div><div class="token-line" style="color:#393A34"><span class="token plain">launch 2 blocks of 2 threads each. The GPU we will do that recursively</span></div><div class="token-line" style="color:#393A34"><span class="token plain">until it reaches max_depth=2</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">In total 2+8=10 blocks are launched!!! (8 from the GPU)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">***************************************************************************</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Launching cdp_kernel() with CUDA Dynamic Parallelism:</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">BLOCK 0 launched by the host</span></div><div class="token-line" style="color:#393A34"><span class="token plain">BLOCK 1 launched by the host</span></div><div class="token-line" style="color:#393A34"><span class="token plain">|  BLOCK 2 launched by thread 0 of block 0</span></div><div class="token-line" style="color:#393A34"><span class="token plain">|  BLOCK 4 launched by thread 0 of block 1</span></div><div class="token-line" style="color:#393A34"><span class="token plain">|  BLOCK 3 launched by thread 0 of block 0</span></div><div class="token-line" style="color:#393A34"><span class="token plain">|  BLOCK 5 launched by thread 0 of block 1</span></div><div class="token-line" style="color:#393A34"><span class="token plain">|  BLOCK 6 launched by thread 1 of block 0</span></div><div class="token-line" style="color:#393A34"><span class="token plain">|  BLOCK 7 launched by thread 1 of block 0</span></div><div class="token-line" style="color:#393A34"><span class="token plain">|  BLOCK 8 launched by thread 1 of block 1</span></div><div class="token-line" style="color:#393A34"><span class="token plain">|  BLOCK 9 launched by thread 1 of block 1</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-cdpsimplequicksort">cdpSimpleQuicksort</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">GPU Device 0: &quot;Ada&quot; with compute capability 8.9</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Initializing data:</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Running quicksort on 128 elements</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Launching kernel on the GPU</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Validating results: OK</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-clock">clock</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">CUDA Clock sample</span></div><div class="token-line" style="color:#393A34"><span class="token plain">GPU Device 0: &quot;Ada&quot; with compute capability 8.9</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Average clocks/block = 2276.953125</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-clock_nvrtc">clock_nvrtc</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">CUDA Clock sample</span></div><div class="token-line" style="color:#393A34"><span class="token plain">&gt; Using CUDA Device [0]: NVIDIA GeForce RTX 4070 Ti</span></div><div class="token-line" style="color:#393A34"><span class="token plain">&gt; Using CUDA Device [0]: NVIDIA GeForce RTX 4070 Ti</span></div><div class="token-line" style="color:#393A34"><span class="token plain">&gt; GPU Device has SM 8.9 compute capability</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Average clocks/block = 2316.890625</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-concurrentkernels">concurrentKernels</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">[./concurrentKernels] - Starting...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">GPU Device 0: &quot;Ada&quot; with compute capability 8.9</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">&gt; Detected Compute SM 8.9 hardware with 60 multi-processors</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Expected time for serial execution of 8 kernels = 0.080s</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Expected time for concurrent execution of 8 kernels = 0.010s</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Measured time for sample = 0.010s</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Test passed</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-conjugategradientmultidevicecg">conjugateGradientMultiDeviceCG</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">Starting [conjugateGradientMultiDeviceCG]...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">GPU Device 0: &quot;NVIDIA GeForce RTX 4070 Ti&quot; with compute capability 8.9</span></div><div class="token-line" style="color:#393A34"><span class="token plain">No two or more GPUs with same architecture capable of concurrentManagedAccess found. </span></div><div class="token-line" style="color:#393A34"><span class="token plain">Waiving the sample</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-convolutionfft2d">convolutionFFT2D</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">[./convolutionFFT2D] - Starting...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">GPU Device 0: &quot;Ada&quot; with compute capability 8.9</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Testing built-in R2C / C2R FFT-based convolution</span></div><div class="token-line" style="color:#393A34"><span class="token plain">...allocating memory</span></div><div class="token-line" style="color:#393A34"><span class="token plain">...generating random input data</span></div><div class="token-line" style="color:#393A34"><span class="token plain">...creating R2C &amp; C2R FFT plans for 2048 x 2048</span></div><div class="token-line" style="color:#393A34"><span class="token plain">...uploading to GPU and padding convolution kernel and input data</span></div><div class="token-line" style="color:#393A34"><span class="token plain">...transforming convolution kernel</span></div><div class="token-line" style="color:#393A34"><span class="token plain">...running GPU FFT convolution: 20100.502416 MPix/s (0.199000 ms)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">...reading back GPU convolution results</span></div><div class="token-line" style="color:#393A34"><span class="token plain">...running reference CPU convolution</span></div><div class="token-line" style="color:#393A34"><span class="token plain">...comparing the results: rel L2 = 9.395370E-08 (max delta = 1.208283E-06)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">L2norm Error OK</span></div><div class="token-line" style="color:#393A34"><span class="token plain">...shutting down</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Testing custom R2C / C2R FFT-based convolution</span></div><div class="token-line" style="color:#393A34"><span class="token plain">...allocating memory</span></div><div class="token-line" style="color:#393A34"><span class="token plain">...generating random input data</span></div><div class="token-line" style="color:#393A34"><span class="token plain">...creating C2C FFT plan for 2048 x 1024</span></div><div class="token-line" style="color:#393A34"><span class="token plain">...uploading to GPU and padding convolution kernel and input data</span></div><div class="token-line" style="color:#393A34"><span class="token plain">...transforming convolution kernel</span></div><div class="token-line" style="color:#393A34"><span class="token plain">...running GPU FFT convolution: 14760.147718 MPix/s (0.271000 ms)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">...reading back GPU FFT results</span></div><div class="token-line" style="color:#393A34"><span class="token plain">...running reference CPU convolution</span></div><div class="token-line" style="color:#393A34"><span class="token plain">...comparing the results: rel L2 = 1.067915E-07 (max delta = 9.817303E-07)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">L2norm Error OK</span></div><div class="token-line" style="color:#393A34"><span class="token plain">...shutting down</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Testing updated custom R2C / C2R FFT-based convolution</span></div><div class="token-line" style="color:#393A34"><span class="token plain">...allocating memory</span></div><div class="token-line" style="color:#393A34"><span class="token plain">...generating random input data</span></div><div class="token-line" style="color:#393A34"><span class="token plain">...creating C2C FFT plan for 2048 x 1024</span></div><div class="token-line" style="color:#393A34"><span class="token plain">...uploading to GPU and padding convolution kernel and input data</span></div><div class="token-line" style="color:#393A34"><span class="token plain">...transforming convolution kernel</span></div><div class="token-line" style="color:#393A34"><span class="token plain">...running GPU FFT convolution: 25477.706155 MPix/s (0.157000 ms)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">...reading back GPU FFT results</span></div><div class="token-line" style="color:#393A34"><span class="token plain">...running reference CPU convolution</span></div><div class="token-line" style="color:#393A34"><span class="token plain">...comparing the results: rel L2 = 1.065127E-07 (max delta = 9.817303E-07)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">L2norm Error OK</span></div><div class="token-line" style="color:#393A34"><span class="token plain">...shutting down</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Test Summary: 0 errors</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Test passed</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-cppintegration">cppIntegration</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">GPU Device 0: &quot;Ada&quot; with compute capability 8.9</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Hello World.</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Hello World.</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-cppoverload">cppOverload</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">C++ Function Overloading starting...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Device Count: 1</span></div><div class="token-line" style="color:#393A34"><span class="token plain">GPU Device 0: &quot;Ada&quot; with compute capability 8.9</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Shared Size:   1024</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Constant Size: 0</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Local Size:    0</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Max Threads Per Block: 1024</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Number of Registers: 12</span></div><div class="token-line" style="color:#393A34"><span class="token plain">PTX Version: 89</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Binary Version: 89</span></div><div class="token-line" style="color:#393A34"><span class="token plain">simple_kernel(const int *pIn, int *pOut, int a) PASSED</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Shared Size:   2048</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Constant Size: 0</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Local Size:    0</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Max Threads Per Block: 1024</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Number of Registers: 12</span></div><div class="token-line" style="color:#393A34"><span class="token plain">PTX Version: 89</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Binary Version: 89</span></div><div class="token-line" style="color:#393A34"><span class="token plain">simple_kernel(const int2 *pIn, int *pOut, int a) PASSED</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Shared Size:   2048</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Constant Size: 0</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Local Size:    0</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Max Threads Per Block: 1024</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Number of Registers: 16</span></div><div class="token-line" style="color:#393A34"><span class="token plain">PTX Version: 89</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Binary Version: 89</span></div><div class="token-line" style="color:#393A34"><span class="token plain">simple_kernel(const int *pIn1, const int *pIn2, int *pOut, int a) PASSED</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-cudacompressiblememory">cudaCompressibleMemory</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">GPU Device 0: &quot;Ada&quot; with compute capability 8.9</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Generic memory compression support is available</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Running saxpy on 167772160 bytes of Compressible memory</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Running saxpy with 120 blocks x 768 threads = 0.188 ms 2.671 TB/s</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Running saxpy on 167772160 bytes of Non-Compressible memory</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Running saxpy with 120 blocks x 768 threads = 1.164 ms 0.432 TB/s</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-cudaopenmp">cudaOpenMP</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">./cudaOpenMP Starting...</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">number of host CPUs:    20</span></div><div class="token-line" style="color:#393A34"><span class="token plain">number of CUDA devices: 1</span></div><div class="token-line" style="color:#393A34"><span class="token plain">   0: NVIDIA GeForce RTX 4070 Ti</span></div><div class="token-line" style="color:#393A34"><span class="token plain">---------------------------</span></div><div class="token-line" style="color:#393A34"><span class="token plain">CPU thread 0 (of 1) uses CUDA device 0</span></div><div class="token-line" style="color:#393A34"><span class="token plain">---------------------------</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-cudatensorcoregemm">cudaTensorCoreGemm</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">Initializing...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">GPU Device 0: &quot;Ada&quot; with compute capability 8.9</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">M: 4096 (16 x 256)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">N: 4096 (16 x 256)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">K: 4096 (16 x 256)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Preparing data for GPU...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Required shared memory size: 64 Kb</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Computing... using high performance kernel compute_gemm </span></div><div class="token-line" style="color:#393A34"><span class="token plain">Time: 2.207872 ms</span></div><div class="token-line" style="color:#393A34"><span class="token plain">TFLOPS: 62.25</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-devicequery">deviceQuery</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">./deviceQuery Starting...</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain"> CUDA Device Query (Runtime API) version (CUDART static linking)</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Detected 1 CUDA Capable device(s)</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Device 0: &quot;NVIDIA GeForce RTX 4070 Ti&quot;</span></div><div class="token-line" style="color:#393A34"><span class="token plain">  CUDA Driver Version / Runtime Version          12.2 / 12.1</span></div><div class="token-line" style="color:#393A34"><span class="token plain">  CUDA Capability Major/Minor version number:    8.9</span></div><div class="token-line" style="color:#393A34"><span class="token plain">  Total amount of global memory:                 11976 MBytes (12557942784 bytes)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">  (060) Multiprocessors, (128) CUDA Cores/MP:    7680 CUDA Cores</span></div><div class="token-line" style="color:#393A34"><span class="token plain">  GPU Max Clock rate:                            2730 MHz (2.73 GHz)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">  Memory Clock rate:                             10501 Mhz</span></div><div class="token-line" style="color:#393A34"><span class="token plain">  Memory Bus Width:                              192-bit</span></div><div class="token-line" style="color:#393A34"><span class="token plain">  L2 Cache Size:                                 50331648 bytes</span></div><div class="token-line" style="color:#393A34"><span class="token plain">  Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers</span></div><div class="token-line" style="color:#393A34"><span class="token plain">  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers</span></div><div class="token-line" style="color:#393A34"><span class="token plain">  Total amount of constant memory:               65536 bytes</span></div><div class="token-line" style="color:#393A34"><span class="token plain">  Total amount of shared memory per block:       49152 bytes</span></div><div class="token-line" style="color:#393A34"><span class="token plain">  Total shared memory per multiprocessor:        102400 bytes</span></div><div class="token-line" style="color:#393A34"><span class="token plain">  Total number of registers available per block: 65536</span></div><div class="token-line" style="color:#393A34"><span class="token plain">  Warp size:                                     32</span></div><div class="token-line" style="color:#393A34"><span class="token plain">  Maximum number of threads per multiprocessor:  1536</span></div><div class="token-line" style="color:#393A34"><span class="token plain">  Maximum number of threads per block:           1024</span></div><div class="token-line" style="color:#393A34"><span class="token plain">  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">  Maximum memory pitch:                          2147483647 bytes</span></div><div class="token-line" style="color:#393A34"><span class="token plain">  Texture alignment:                             512 bytes</span></div><div class="token-line" style="color:#393A34"><span class="token plain">  Concurrent copy and kernel execution:          Yes with 2 copy engine(s)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">  Run time limit on kernels:                     Yes</span></div><div class="token-line" style="color:#393A34"><span class="token plain">  Integrated GPU sharing Host Memory:            No</span></div><div class="token-line" style="color:#393A34"><span class="token plain">  Support host page-locked memory mapping:       Yes</span></div><div class="token-line" style="color:#393A34"><span class="token plain">  Alignment requirement for Surfaces:            Yes</span></div><div class="token-line" style="color:#393A34"><span class="token plain">  Device has ECC support:                        Disabled</span></div><div class="token-line" style="color:#393A34"><span class="token plain">  Device supports Unified Addressing (UVA):      Yes</span></div><div class="token-line" style="color:#393A34"><span class="token plain">  Device supports Managed Memory:                Yes</span></div><div class="token-line" style="color:#393A34"><span class="token plain">  Device supports Compute Preemption:            Yes</span></div><div class="token-line" style="color:#393A34"><span class="token plain">  Supports Cooperative Kernel Launch:            Yes</span></div><div class="token-line" style="color:#393A34"><span class="token plain">  Supports MultiDevice Co-op Kernel Launch:      Yes</span></div><div class="token-line" style="color:#393A34"><span class="token plain">  Device PCI Domain ID / Bus ID / location ID:   0 / 1 / 0</span></div><div class="token-line" style="color:#393A34"><span class="token plain">  Compute Mode:</span></div><div class="token-line" style="color:#393A34"><span class="token plain">     &lt; Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) &gt;</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 12.2, CUDA Runtime Version = 12.1, NumDevs = 1</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Result = PASS</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-devicequerydrv">deviceQueryDrv</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">./deviceQueryDrv Starting...</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">CUDA Device Query (Driver API) statically linked version </span></div><div class="token-line" style="color:#393A34"><span class="token plain">Detected 1 CUDA Capable device(s)</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Device 0: &quot;NVIDIA GeForce RTX 4070 Ti&quot;</span></div><div class="token-line" style="color:#393A34"><span class="token plain">  CUDA Driver Version:                           12.2</span></div><div class="token-line" style="color:#393A34"><span class="token plain">  CUDA Capability Major/Minor version number:    8.9</span></div><div class="token-line" style="color:#393A34"><span class="token plain">  Total amount of global memory:                 11976 MBytes (12557942784 bytes)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">  (60) Multiprocessors, (128) CUDA Cores/MP:     7680 CUDA Cores</span></div><div class="token-line" style="color:#393A34"><span class="token plain">  GPU Max Clock rate:                            2730 MHz (2.73 GHz)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">  Memory Clock rate:                             10501 Mhz</span></div><div class="token-line" style="color:#393A34"><span class="token plain">  Memory Bus Width:                              192-bit</span></div><div class="token-line" style="color:#393A34"><span class="token plain">  L2 Cache Size:                                 50331648 bytes</span></div><div class="token-line" style="color:#393A34"><span class="token plain">  Max Texture Dimension Sizes                    1D=(131072) 2D=(131072, 65536) 3D=(16384, 16384, 16384)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers</span></div><div class="token-line" style="color:#393A34"><span class="token plain">  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers</span></div><div class="token-line" style="color:#393A34"><span class="token plain">  Total amount of constant memory:               65536 bytes</span></div><div class="token-line" style="color:#393A34"><span class="token plain">  Total amount of shared memory per block:       49152 bytes</span></div><div class="token-line" style="color:#393A34"><span class="token plain">  Total number of registers available per block: 65536</span></div><div class="token-line" style="color:#393A34"><span class="token plain">  Warp size:                                     32</span></div><div class="token-line" style="color:#393A34"><span class="token plain">  Maximum number of threads per multiprocessor:  1536</span></div><div class="token-line" style="color:#393A34"><span class="token plain">  Maximum number of threads per block:           1024</span></div><div class="token-line" style="color:#393A34"><span class="token plain">  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">  Max dimension size of a grid size (x,y,z):    (2147483647, 65535, 65535)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">  Texture alignment:                             512 bytes</span></div><div class="token-line" style="color:#393A34"><span class="token plain">  Maximum memory pitch:                          2147483647 bytes</span></div><div class="token-line" style="color:#393A34"><span class="token plain">  Concurrent copy and kernel execution:          Yes with 2 copy engine(s)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">  Run time limit on kernels:                     Yes</span></div><div class="token-line" style="color:#393A34"><span class="token plain">  Integrated GPU sharing Host Memory:            No</span></div><div class="token-line" style="color:#393A34"><span class="token plain">  Support host page-locked memory mapping:       Yes</span></div><div class="token-line" style="color:#393A34"><span class="token plain">  Concurrent kernel execution:                   Yes</span></div><div class="token-line" style="color:#393A34"><span class="token plain">  Alignment requirement for Surfaces:            Yes</span></div><div class="token-line" style="color:#393A34"><span class="token plain">  Device has ECC support:                        Disabled</span></div><div class="token-line" style="color:#393A34"><span class="token plain">  Device supports Unified Addressing (UVA):      Yes</span></div><div class="token-line" style="color:#393A34"><span class="token plain">  Device supports Managed Memory:                Yes</span></div><div class="token-line" style="color:#393A34"><span class="token plain">  Device supports Compute Preemption:            Yes</span></div><div class="token-line" style="color:#393A34"><span class="token plain">  Supports Cooperative Kernel Launch:            Yes</span></div><div class="token-line" style="color:#393A34"><span class="token plain">  Supports MultiDevice Co-op Kernel Launch:      Yes</span></div><div class="token-line" style="color:#393A34"><span class="token plain">  Device PCI Domain ID / Bus ID / location ID:   0 / 1 / 0</span></div><div class="token-line" style="color:#393A34"><span class="token plain">  Compute Mode:</span></div><div class="token-line" style="color:#393A34"><span class="token plain">     &lt; Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) &gt;</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Result = PASS</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-dmmatensorcoregemm">dmmaTensorCoreGemm</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">Initializing...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">GPU Device 0: &quot;Ada&quot; with compute capability 8.9</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">M: 8192 (8 x 1024)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">N: 8192 (8 x 1024)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">K: 4096 (4 x 1024)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Preparing data for GPU...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Required shared memory size: 68 Kb</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Computing using high performance kernel = 0 - compute_dgemm_async_copy</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Time: 942.316528 ms</span></div><div class="token-line" style="color:#393A34"><span class="token plain">FP64 TFLOPS: 0.58</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-dwthaar1d">dwtHaar1D</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">./dwtHaar1D Starting...</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">GPU Device 0: &quot;Ada&quot; with compute capability 8.9</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">source file    = &quot;../../../../Samples/5_Domain_Specific/dwtHaar1D/data/signal.dat&quot;</span></div><div class="token-line" style="color:#393A34"><span class="token plain">reference file = &quot;result.dat&quot;</span></div><div class="token-line" style="color:#393A34"><span class="token plain">gold file      = &quot;../../../../Samples/5_Domain_Specific/dwtHaar1D/data/regression.gold.dat&quot;</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Reading signal from &quot;../../../../Samples/5_Domain_Specific/dwtHaar1D/data/signal.dat&quot;</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Writing result to &quot;result.dat&quot;</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Reading reference result from &quot;../../../../Samples/5_Domain_Specific/dwtHaar1D/data/regression.gold.dat&quot;</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Test success!</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-dxtc">dxtc</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">./dxtc Starting...</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">GPU Device 0: &quot;Ada&quot; with compute capability 8.9</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Image Loaded &#x27;../../../../Samples/5_Domain_Specific/dxtc/data/teapot512_std.ppm&#x27;, 512 x 512 pixels</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Running DXT Compression on 512 x 512 image...</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">16384 Blocks, 64 Threads per Block, 1048576 Threads in Grid...</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">dxtc, Throughput = 530.6559 MPixels/s, Time = 0.00049 s, Size = 262144 Pixels, NumDevsUsed = 1, Workgroup = 64</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Checking accuracy...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">RMS(reference, result) = 0.000000</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Test passed</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-encode_output">encode_output</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">./markdown_echo_for.sh: line 10: ./encode_output: Is a directory</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-fastwalshtransform">fastWalshTransform</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">./fastWalshTransform Starting...</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">GPU Device 0: &quot;Ada&quot; with compute capability 8.9</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Initializing data...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">...allocating CPU memory</span></div><div class="token-line" style="color:#393A34"><span class="token plain">...allocating GPU memory</span></div><div class="token-line" style="color:#393A34"><span class="token plain">...generating data</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Data length: 8388608; kernel length: 128</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Running GPU dyadic convolution using Fast Walsh Transform...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">GPU time: 1.171000 ms; GOP/s: 247.145154</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Reading back GPU results...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Running straightforward CPU dyadic convolution...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Comparing the results...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Shutting down...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">L2 norm: 1.021579E-07</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Test passed</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-fdtd3d">FDTD3d</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">./FDTD3d Starting...</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Set-up, based upon target device GMEM size...</span></div><div class="token-line" style="color:#393A34"><span class="token plain"> getTargetDeviceGlobalMemSize</span></div><div class="token-line" style="color:#393A34"><span class="token plain"> cudaGetDeviceCount</span></div><div class="token-line" style="color:#393A34"><span class="token plain">GPU Device 0: &quot;Ada&quot; with compute capability 8.9</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain"> cudaGetDeviceProperties</span></div><div class="token-line" style="color:#393A34"><span class="token plain"> generateRandomData</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">FDTD on 376 x 376 x 376 volume with symmetric filter radius 4 for 5 timesteps...</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">fdtdReference...</span></div><div class="token-line" style="color:#393A34"><span class="token plain"> calloc intermediate</span></div><div class="token-line" style="color:#393A34"><span class="token plain"> Host FDTD loop</span></div><div class="token-line" style="color:#393A34"><span class="token plain">    t = 0</span></div><div class="token-line" style="color:#393A34"><span class="token plain">    t = 1</span></div><div class="token-line" style="color:#393A34"><span class="token plain">    t = 2</span></div><div class="token-line" style="color:#393A34"><span class="token plain">    t = 3</span></div><div class="token-line" style="color:#393A34"><span class="token plain">    t = 4</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">fdtdReference complete</span></div><div class="token-line" style="color:#393A34"><span class="token plain">fdtdGPU...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">GPU Device 0: &quot;Ada&quot; with compute capability 8.9</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain"> set block size to 32x16</span></div><div class="token-line" style="color:#393A34"><span class="token plain"> set grid size to 12x24</span></div><div class="token-line" style="color:#393A34"><span class="token plain"> GPU FDTD loop</span></div><div class="token-line" style="color:#393A34"><span class="token plain">    t = 0 launch kernel</span></div><div class="token-line" style="color:#393A34"><span class="token plain">    t = 1 launch kernel</span></div><div class="token-line" style="color:#393A34"><span class="token plain">    t = 2 launch kernel</span></div><div class="token-line" style="color:#393A34"><span class="token plain">    t = 3 launch kernel</span></div><div class="token-line" style="color:#393A34"><span class="token plain">    t = 4 launch kernel</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">fdtdGPU complete</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">CompareData (tolerance 0.000100)...</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-fp16scalarproduct">fp16ScalarProduct</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">GPU Device 0: &quot;Ada&quot; with compute capability 8.9</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Result native operators : 658990.000000 </span></div><div class="token-line" style="color:#393A34"><span class="token plain">Result intrinsics   : 658990.000000 </span></div><div class="token-line" style="color:#393A34"><span class="token plain">&amp;&amp;&amp;&amp; fp16ScalarProduct PASSED</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-globaltoshmemasynccopy">globalToShmemAsyncCopy</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">[globalToShmemAsyncCopy] - Starting...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">GPU Device 0: &quot;Ada&quot; with compute capability 8.9</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">MatrixA(1280,1280), MatrixB(1280,1280)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Running kernel = 0 - AsyncCopyMultiStageLargeChunk</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Computing result using CUDA Kernel...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">done</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Performance= 2897.51 GFlop/s, Time= 1.448 msec, Size= 4194304000 Ops, WorkgroupSize= 256 threads/block</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Checking computed result for correctness: Result = PASS</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-graphmemoryfootprint">graphMemoryFootprint</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">GPU Device 0: &quot;Ada&quot; with compute capability 8.9</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Driver version is: 12.2</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Running sample.</span></div><div class="token-line" style="color:#393A34"><span class="token plain">================================</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Running virtual address reuse example.</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Sequential allocations &amp; frees within a single graph enable CUDA to reuse virtual addresses.</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Check confirms that d_a and d_b share a virtual address.</span></div><div class="token-line" style="color:#393A34"><span class="token plain">    FOOTPRINT: 67108864 bytes</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Cleaning up example by trimming device memory.</span></div><div class="token-line" style="color:#393A34"><span class="token plain">    FOOTPRINT: 0 bytes</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">================================</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Running physical memory reuse example.</span></div><div class="token-line" style="color:#393A34"><span class="token plain">CUDA reuses the same physical memory for allocations from separate graphs when the allocation lifetimes don&#x27;t overlap.</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Creating the graph execs does not reserve any physical memory.</span></div><div class="token-line" style="color:#393A34"><span class="token plain">    FOOTPRINT: 0 bytes</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">The first graph launched reserves the memory it needs.</span></div><div class="token-line" style="color:#393A34"><span class="token plain">    FOOTPRINT: 67108864 bytes</span></div><div class="token-line" style="color:#393A34"><span class="token plain">A subsequent launch of the same graph in the same stream reuses the same physical memory. Thus the memory footprint does not grow here.</span></div><div class="token-line" style="color:#393A34"><span class="token plain">    FOOTPRINT: 67108864 bytes</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Subsequent launches of other graphs in the same stream also reuse the physical memory. Thus the memory footprint does not grow here.</span></div><div class="token-line" style="color:#393A34"><span class="token plain">01:     FOOTPRINT: 67108864 bytes</span></div><div class="token-line" style="color:#393A34"><span class="token plain">02:     FOOTPRINT: 67108864 bytes</span></div><div class="token-line" style="color:#393A34"><span class="token plain">03:     FOOTPRINT: 67108864 bytes</span></div><div class="token-line" style="color:#393A34"><span class="token plain">04:     FOOTPRINT: 67108864 bytes</span></div><div class="token-line" style="color:#393A34"><span class="token plain">05:     FOOTPRINT: 67108864 bytes</span></div><div class="token-line" style="color:#393A34"><span class="token plain">06:     FOOTPRINT: 67108864 bytes</span></div><div class="token-line" style="color:#393A34"><span class="token plain">07:     FOOTPRINT: 67108864 bytes</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Check confirms all graphs use a different virtual address.</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Cleaning up example by trimming device memory.</span></div><div class="token-line" style="color:#393A34"><span class="token plain">    FOOTPRINT: 0 bytes</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">================================</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Running simultaneous streams example.</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Graphs that can run concurrently need separate physical memory. In this example, each graph launched in a separate stream increases the total memory footprint.</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">When launching a new graph, CUDA may reuse physical memory from a graph whose execution has already finished -- even if the new graph is being launched in a different stream from the completed graph. Therefore, a kernel node is added to the graphs to increase runtime.</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Initial footprint:</span></div><div class="token-line" style="color:#393A34"><span class="token plain">    FOOTPRINT: 0 bytes</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Each graph launch in a seperate stream grows the memory footprint:</span></div><div class="token-line" style="color:#393A34"><span class="token plain">01:     FOOTPRINT: 67108864 bytes</span></div><div class="token-line" style="color:#393A34"><span class="token plain">02:     FOOTPRINT: 134217728 bytes</span></div><div class="token-line" style="color:#393A34"><span class="token plain">03:     FOOTPRINT: 201326592 bytes</span></div><div class="token-line" style="color:#393A34"><span class="token plain">04:     FOOTPRINT: 268435456 bytes</span></div><div class="token-line" style="color:#393A34"><span class="token plain">05:     FOOTPRINT: 335544320 bytes</span></div><div class="token-line" style="color:#393A34"><span class="token plain">06:     FOOTPRINT: 402653184 bytes</span></div><div class="token-line" style="color:#393A34"><span class="token plain">07:     FOOTPRINT: 469762048 bytes</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Cleaning up example by trimming device memory.</span></div><div class="token-line" style="color:#393A34"><span class="token plain">    FOOTPRINT: 0 bytes</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">================================</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Running unfreed streams example.</span></div><div class="token-line" style="color:#393A34"><span class="token plain">CUDA cannot reuse phyiscal memory from graphs which do not free their allocations.</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Despite being launched in the same stream, each graph launch grows the memory footprint. Since the allocation is not freed, CUDA keeps the memory valid for use.</span></div><div class="token-line" style="color:#393A34"><span class="token plain">00:     FOOTPRINT: 67108864 bytes</span></div><div class="token-line" style="color:#393A34"><span class="token plain">01:     FOOTPRINT: 134217728 bytes</span></div><div class="token-line" style="color:#393A34"><span class="token plain">02:     FOOTPRINT: 201326592 bytes</span></div><div class="token-line" style="color:#393A34"><span class="token plain">03:     FOOTPRINT: 268435456 bytes</span></div><div class="token-line" style="color:#393A34"><span class="token plain">04:     FOOTPRINT: 335544320 bytes</span></div><div class="token-line" style="color:#393A34"><span class="token plain">05:     FOOTPRINT: 402653184 bytes</span></div><div class="token-line" style="color:#393A34"><span class="token plain">06:     FOOTPRINT: 469762048 bytes</span></div><div class="token-line" style="color:#393A34"><span class="token plain">07:     FOOTPRINT: 536870912 bytes</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Trimming does not impact the memory footprint since the un-freed allocations are still holding onto the memory.</span></div><div class="token-line" style="color:#393A34"><span class="token plain">    FOOTPRINT: 536870912 bytes</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Freeing the allocations does not shrink the footprint.</span></div><div class="token-line" style="color:#393A34"><span class="token plain">    FOOTPRINT: 536870912 bytes</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Since the allocations are now freed, trimming does reduce the footprint even when the graph execs are not yet destroyed.</span></div><div class="token-line" style="color:#393A34"><span class="token plain">    FOOTPRINT: 0 bytes</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Cleaning up example by trimming device memory.</span></div><div class="token-line" style="color:#393A34"><span class="token plain">    FOOTPRINT: 0 bytes</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">================================</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Sample complete.</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-graphmemorynodes">graphMemoryNodes</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">GPU Device 0: &quot;Ada&quot; with compute capability 8.9</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Driver version is: 12.2</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Setting up sample.</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Setup complete.</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Running negateSquares in a stream.</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Validating negateSquares in a stream...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Validation PASSED!</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Running negateSquares in a stream-captured graph.</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Validating negateSquares in a stream-captured graph...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Validation PASSED!</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Running negateSquares in an explicitly constructed graph.</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Check verified that d_negSquare and d_input share a virtual address.</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Validating negateSquares in an explicitly constructed graph...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Validation PASSED!</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Running negateSquares with d_negSquare freed outside the stream.</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Check verified that d_negSquare and d_input share a virtual address.</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Validating negateSquares with d_negSquare freed outside the stream...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Validation PASSED!</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Running negateSquares with d_negSquare freed outside the graph.</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Validating negateSquares with d_negSquare freed outside the graph...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Validation PASSED!</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Running negateSquares with d_negSquare freed in a different graph.</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Validating negateSquares with d_negSquare freed in a different graph...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Validation PASSED!</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Cleaning up sample.</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Cleanup complete. Exiting sample.</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-hsopticalflow">HSOpticalFlow</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">HSOpticalFlow Starting...</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">GPU Device 0: &quot;Ada&quot; with compute capability 8.9</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Loading &quot;frame10.ppm&quot; ...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Loading &quot;frame11.ppm&quot; ...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Computing optical flow on CPU...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Computing optical flow on GPU...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">L1 error : 0.044308</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-immatensorcoregemm">immaTensorCoreGemm</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">Initializing...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">GPU Device 0: &quot;Ada&quot; with compute capability 8.9</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">M: 4096 (16 x 256)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">N: 4096 (16 x 256)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">K: 4096 (16 x 256)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Preparing data for GPU...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Required shared memory size: 64 Kb</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Computing... using high performance kernel compute_gemm_imma </span></div><div class="token-line" style="color:#393A34"><span class="token plain">Time: 0.983040 ms</span></div><div class="token-line" style="color:#393A34"><span class="token plain">TOPS: 139.81</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-jacobicudagraphs">jacobiCudaGraphs</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">GPU Device 0: &quot;Ada&quot; with compute capability 8.9</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">CPU iterations : 2954</span></div><div class="token-line" style="color:#393A34"><span class="token plain">CPU error : 4.988e-03</span></div><div class="token-line" style="color:#393A34"><span class="token plain">CPU Processing time: 1204.943970 (ms)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">GPU iterations : 2954</span></div><div class="token-line" style="color:#393A34"><span class="token plain">GPU error : 4.988e-03</span></div><div class="token-line" style="color:#393A34"><span class="token plain">GPU Processing time: 63.724998 (ms)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">&amp;&amp;&amp;&amp; jacobiCudaGraphs PASSED</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-matrixmul">matrixMul</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">[Matrix Multiply Using CUDA] - Starting...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">GPU Device 0: &quot;Ada&quot; with compute capability 8.9</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">MatrixA(320,320), MatrixB(640,320)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Computing result using CUDA Kernel...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">done</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Performance= 2698.52 GFlop/s, Time= 0.049 msec, Size= 131072000 Ops, WorkgroupSize= 1024 threads/block</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Checking computed result for correctness: Result = PASS</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-matrixmuldrv">matrixMulDrv</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">[ matrixMulDrv (Driver API) ]</span></div><div class="token-line" style="color:#393A34"><span class="token plain">&gt; Using CUDA Device [0]: NVIDIA GeForce RTX 4070 Ti</span></div><div class="token-line" style="color:#393A34"><span class="token plain">&gt; GPU Device has SM 8.9 compute capability</span></div><div class="token-line" style="color:#393A34"><span class="token plain">  Total amount of global memory:     12557942784 bytes</span></div><div class="token-line" style="color:#393A34"><span class="token plain">&gt; findModulePath found file at &lt;./matrixMul_kernel64.fatbin&gt;</span></div><div class="token-line" style="color:#393A34"><span class="token plain">&gt; initCUDA loading module: &lt;./matrixMul_kernel64.fatbin&gt;</span></div><div class="token-line" style="color:#393A34"><span class="token plain">&gt; 16 block size selected</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Processing time: 0.023000 (ms)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Checking computed result for correctness: Result = PASS</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-matrixmuldynlinkjit">matrixMulDynlinkJIT</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">[ matrixMulDynlinkJIT (CUDA dynamic linking) ]</span></div><div class="token-line" style="color:#393A34"><span class="token plain">&gt; Device 0: &quot;NVIDIA GeForce RTX 4070 Ti&quot; with Compute 8.9 capability</span></div><div class="token-line" style="color:#393A34"><span class="token plain">&gt; Compiling CUDA module</span></div><div class="token-line" style="color:#393A34"><span class="token plain">&gt; PTX JIT log:</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Test run success!</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-matrixmul_nvrtc">matrixMul_nvrtc</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">[Matrix Multiply Using CUDA] - Starting...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">MatrixA(320,320), MatrixB(640,320)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">&gt; Using CUDA Device [0]: NVIDIA GeForce RTX 4070 Ti</span></div><div class="token-line" style="color:#393A34"><span class="token plain">&gt; Using CUDA Device [0]: NVIDIA GeForce RTX 4070 Ti</span></div><div class="token-line" style="color:#393A34"><span class="token plain">&gt; GPU Device has SM 8.9 compute capability</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Computing result using CUDA Kernel...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Checking computed result for correctness: Result = PASS</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-memmapipcdrv">memMapIPCDrv</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">&gt; findModulePath found file at &lt;./memMapIpc_kernel64.ptx&gt;</span></div><div class="token-line" style="color:#393A34"><span class="token plain">&gt; initCUDA loading module: &lt;./memMapIpc_kernel64.ptx&gt;</span></div><div class="token-line" style="color:#393A34"><span class="token plain">&gt; PTX JIT log:</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Step 0 done</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Process 0: verifying...</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-mergesort">mergeSort</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">./mergeSort Starting...</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">GPU Device 0: &quot;Ada&quot; with compute capability 8.9</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Allocating and initializing host arrays...</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Allocating and initializing CUDA arrays...</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Initializing GPU merge sort...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Running GPU merge sort...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Time: 2.122000 ms</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Reading back GPU merge sort results...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Inspecting the results...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">...inspecting keys array: OK</span></div><div class="token-line" style="color:#393A34"><span class="token plain">...inspecting keys and values array: OK</span></div><div class="token-line" style="color:#393A34"><span class="token plain">...stability property: stable!</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Shutting down...</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-montecarlomultigpu">MonteCarloMultiGPU</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">./MonteCarloMultiGPU Starting...</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Using single CPU thread for multiple GPUs</span></div><div class="token-line" style="color:#393A34"><span class="token plain">MonteCarloMultiGPU</span></div><div class="token-line" style="color:#393A34"><span class="token plain">==================</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Parallelization method  = streamed</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Problem scaling         = weak</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Number of GPUs          = 1</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Total number of options = 8192</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Number of paths         = 262144</span></div><div class="token-line" style="color:#393A34"><span class="token plain">main(): generating input data...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">main(): starting 1 host threads...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">main(): GPU statistics, streamed</span></div><div class="token-line" style="color:#393A34"><span class="token plain">GPU Device #0: NVIDIA GeForce RTX 4070 Ti</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Options         : 8192</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Simulation paths: 262144</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Total time (ms.): 6.029000</span></div><div class="token-line" style="color:#393A34"><span class="token plain">    Note: This is elapsed time for all to compute.</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Options per sec.: 1358766.008351</span></div><div class="token-line" style="color:#393A34"><span class="token plain">main(): comparing Monte Carlo and Black-Scholes results...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Shutting down...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Test Summary...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">L1 norm        : 4.898407E-04</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Average reserve: 12.983048</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Test passed</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-newdelete">newdelete</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">newdelete Starting...</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">GPU Device 0: &quot;Ada&quot; with compute capability 8.9</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain"> &gt; Container = Vector test OK</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain"> &gt; Container = Vector, using placement new on SMEM buffer test OK</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain"> &gt; Container = Vector, with user defined datatype test OK</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Test Summary: 3/3 succesfully run</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-nv12tobgrandresize">NV12toBGRandResize</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">GPU Device 0: &quot;Ada&quot; with compute capability 8.9</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">TEST#1:</span></div><div class="token-line" style="color:#393A34"><span class="token plain">  CUDA resize nv12(1920x1080 --&gt; 640x480), batch: 24, average time: 0.036 ms ==&gt; 0.001 ms/frame</span></div><div class="token-line" style="color:#393A34"><span class="token plain">  CUDA convert nv12(640x480) to bgr(640x480), batch: 24, average time: 0.230 ms ==&gt; 0.010 ms/frame</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">TEST#2:</span></div><div class="token-line" style="color:#393A34"><span class="token plain">  CUDA convert nv12(1920x1080) to bgr(1920x1080), batch: 24, average time: 1.630 ms ==&gt; 0.068 ms/frame</span></div><div class="token-line" style="color:#393A34"><span class="token plain">  CUDA resize bgr(1920x1080 --&gt; 640x480), batch: 24, average time: 1.223 ms ==&gt; 0.051 ms/frame</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-output">output</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">./markdown_echo_for.sh: line 10: ./output: Is a directory</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-p2pbandwidthlatencytest">p2pBandwidthLatencyTest</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">[P2P (Peer-to-Peer) GPU Bandwidth Latency Test]</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Device: 0, NVIDIA GeForce RTX 4070 Ti, pciBusID: 1, pciDeviceID: 0, pciDomainID:0</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">***NOTE: In case a device doesn&#x27;t have P2P access to other one, it falls back to normal memcopy procedure.</span></div><div class="token-line" style="color:#393A34"><span class="token plain">So you can see lesser Bandwidth (GB/s) and unstable Latency (us) in those cases.</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">P2P Connectivity Matrix</span></div><div class="token-line" style="color:#393A34"><span class="token plain">     D\D     0</span></div><div class="token-line" style="color:#393A34"><span class="token plain">     0       1</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Unidirectional P2P=Disabled Bandwidth Matrix (GB/s)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">   D\D     0 </span></div><div class="token-line" style="color:#393A34"><span class="token plain">     0 428.20 </span></div><div class="token-line" style="color:#393A34"><span class="token plain">Unidirectional P2P=Enabled Bandwidth (P2P Writes) Matrix (GB/s)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">   D\D     0 </span></div><div class="token-line" style="color:#393A34"><span class="token plain">     0 389.94 </span></div><div class="token-line" style="color:#393A34"><span class="token plain">Bidirectional P2P=Disabled Bandwidth Matrix (GB/s)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">   D\D     0 </span></div><div class="token-line" style="color:#393A34"><span class="token plain">     0 387.46 </span></div><div class="token-line" style="color:#393A34"><span class="token plain">Bidirectional P2P=Enabled Bandwidth Matrix (GB/s)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">   D\D     0 </span></div><div class="token-line" style="color:#393A34"><span class="token plain">     0 390.17 </span></div><div class="token-line" style="color:#393A34"><span class="token plain">P2P=Disabled Latency Matrix (us)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">   GPU     0 </span></div><div class="token-line" style="color:#393A34"><span class="token plain">     0   1.20 </span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">   CPU     0 </span></div><div class="token-line" style="color:#393A34"><span class="token plain">     0   1.11 </span></div><div class="token-line" style="color:#393A34"><span class="token plain">P2P=Enabled Latency (P2P Writes) Matrix (us)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">   GPU     0 </span></div><div class="token-line" style="color:#393A34"><span class="token plain">     0   1.16 </span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">   CPU     0 </span></div><div class="token-line" style="color:#393A34"><span class="token plain">     0   1.08 </span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-ptxjit">ptxjit</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">[PTX Just In Time (JIT) Compilation (no-qatest)] - Starting...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">&gt; Using CUDA Device [0]: NVIDIA GeForce RTX 4070 Ti</span></div><div class="token-line" style="color:#393A34"><span class="token plain">&gt; findModulePath &lt;./ptxjit_kernel64.ptx&gt;</span></div><div class="token-line" style="color:#393A34"><span class="token plain">&gt; initCUDA loading module: &lt;./ptxjit_kernel64.ptx&gt;</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Loading ptxjit_kernel[] program</span></div><div class="token-line" style="color:#393A34"><span class="token plain">CUDA Link Completed in 0.000000ms. Linker Output:</span></div><div class="token-line" style="color:#393A34"><span class="token plain">ptxas info    : 0 bytes gmem</span></div><div class="token-line" style="color:#393A34"><span class="token plain">ptxas info    : Compiling entry function &#x27;myKernel&#x27; for &#x27;sm_89&#x27;</span></div><div class="token-line" style="color:#393A34"><span class="token plain">ptxas info    : Function properties for myKernel</span></div><div class="token-line" style="color:#393A34"><span class="token plain">ptxas         .     0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads</span></div><div class="token-line" style="color:#393A34"><span class="token plain">ptxas info    : Used 8 registers, 360 bytes cmem[0]</span></div><div class="token-line" style="color:#393A34"><span class="token plain">info    : 0 bytes gmem</span></div><div class="token-line" style="color:#393A34"><span class="token plain">info    : Function properties for &#x27;myKernel&#x27;:</span></div><div class="token-line" style="color:#393A34"><span class="token plain">info    : used 8 registers, 0 stack, 0 bytes smem, 360 bytes cmem[0], 0 bytes lmem</span></div><div class="token-line" style="color:#393A34"><span class="token plain">CUDA kernel launched</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-quasirandomgenerator">quasirandomGenerator</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">./quasirandomGenerator Starting...</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Allocating GPU memory...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Allocating CPU memory...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Initializing QRNG tables...</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Testing QRNG...</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">quasirandomGenerator, Throughput = 59.8047 GNumbers/s, Time = 0.00005 s, Size = 3145728 Numbers, NumDevsUsed = 1, Workgroup = 384</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Reading GPU results...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Comparing to the CPU results...</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">L1 norm: 7.275964E-12</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Testing inverseCNDgpu()...</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">quasirandomGenerator-inverse, Throughput = 186.6901 GNumbers/s, Time = 0.00002 s, Size = 3145728 Numbers, NumDevsUsed = 1, Workgroup = 128</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Reading GPU results...</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Comparing to the CPU results...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">L1 norm: 9.439909E-08</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Shutting down...</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-quasirandomgenerator_nvrtc">quasirandomGenerator_nvrtc</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">./quasirandomGenerator_nvrtc Starting...</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">&gt; Using CUDA Device [0]: NVIDIA GeForce RTX 4070 Ti</span></div><div class="token-line" style="color:#393A34"><span class="token plain">&gt; Using CUDA Device [0]: NVIDIA GeForce RTX 4070 Ti</span></div><div class="token-line" style="color:#393A34"><span class="token plain">&gt; GPU Device has SM 8.9 compute capability</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Allocating GPU memory...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Allocating CPU memory...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Initializing QRNG tables...</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Testing QRNG...</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">quasirandomGenerator, Throughput = 57.5614 GNumbers/s, Time = 0.00005 s, Size = 3145728 Numbers, NumDevsUsed = 1, Workgroup = 384</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Reading GPU results...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Comparing to the CPU results...</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">L1 norm: 7.275964E-12</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Testing inverseCNDgpu()...</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">quasirandomGenerator-inverse, Throughput = 162.9911 GNumbers/s, Time = 0.00002 s, Size = 3145728 Numbers, NumDevsUsed = 1, Workgroup = 128</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Reading GPU results...</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Comparing to the CPU results...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">L1 norm: 9.439909E-08</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Shutting down...</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-simpleassert">simpleAssert</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">simpleAssert.cu:63: void testKernel(int): block: [1,0,0], thread: [28,0,0] Assertion `gtid &lt; N` failed.</span></div><div class="token-line" style="color:#393A34"><span class="token plain">simpleAssert.cu:63: void testKernel(int): block: [1,0,0], thread: [29,0,0] Assertion `gtid &lt; N` failed.</span></div><div class="token-line" style="color:#393A34"><span class="token plain">simpleAssert.cu:63: void testKernel(int): block: [1,0,0], thread: [30,0,0] Assertion `gtid &lt; N` failed.</span></div><div class="token-line" style="color:#393A34"><span class="token plain">simpleAssert.cu:63: void testKernel(int): block: [1,0,0], thread: [31,0,0] Assertion `gtid &lt; N` failed.</span></div><div class="token-line" style="color:#393A34"><span class="token plain">simpleAssert starting...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">OS_System_Type.release = 5.15.0-82-generic</span></div><div class="token-line" style="color:#393A34"><span class="token plain">OS Info: &lt;#91~20.04.1-Ubuntu SMP Fri Aug 18 16:24:39 UTC 2023&gt;</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">GPU Device 0: &quot;Ada&quot; with compute capability 8.9</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Launch kernel to generate assertion failures</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">-- Begin assert output</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">-- End assert output</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Device assert failed as expected, CUDA error message is: device-side assert triggered</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">simpleAssert completed, returned OK</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-simpleassert_nvrtc">simpleAssert_nvrtc</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">../../../../Samples/0_Introduction/simpleAssert_nvrtc/simpleAssert_kernel.cu:37: void testKernel(int): block: [1,0,0], thread: [28,0,0] Assertion `gtid &lt; N` failed.</span></div><div class="token-line" style="color:#393A34"><span class="token plain">../../../../Samples/0_Introduction/simpleAssert_nvrtc/simpleAssert_kernel.cu:37: void testKernel(int): block: [1,0,0], thread: [29,0,0] Assertion `gtid &lt; N` failed.</span></div><div class="token-line" style="color:#393A34"><span class="token plain">../../../../Samples/0_Introduction/simpleAssert_nvrtc/simpleAssert_kernel.cu:37: void testKernel(int): block: [1,0,0], thread: [30,0,0] Assertion `gtid &lt; N` failed.</span></div><div class="token-line" style="color:#393A34"><span class="token plain">../../../../Samples/0_Introduction/simpleAssert_nvrtc/simpleAssert_kernel.cu:37: void testKernel(int): block: [1,0,0], thread: [31,0,0] Assertion `gtid &lt; N` failed.</span></div><div class="token-line" style="color:#393A34"><span class="token plain">simpleAssert_nvrtc starting...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Launch kernel to generate assertion failures</span></div><div class="token-line" style="color:#393A34"><span class="token plain">&gt; Using CUDA Device [0]: NVIDIA GeForce RTX 4070 Ti</span></div><div class="token-line" style="color:#393A34"><span class="token plain">&gt; Using CUDA Device [0]: NVIDIA GeForce RTX 4070 Ti</span></div><div class="token-line" style="color:#393A34"><span class="token plain">&gt; GPU Device has SM 8.9 compute capability</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">-- Begin assert output</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">-- End assert output</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Device assert failed as expected</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-simpleatomicintrinsics">simpleAtomicIntrinsics</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">simpleAtomicIntrinsics starting...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">GPU Device 0: &quot;Ada&quot; with compute capability 8.9</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Processing time: 0.564000 (ms)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">simpleAtomicIntrinsics completed, returned OK</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-simpleatomicintrinsics_nvrtc">simpleAtomicIntrinsics_nvrtc</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">simpleAtomicIntrinsics_nvrtc starting...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">&gt; Using CUDA Device [0]: NVIDIA GeForce RTX 4070 Ti</span></div><div class="token-line" style="color:#393A34"><span class="token plain">&gt; Using CUDA Device [0]: NVIDIA GeForce RTX 4070 Ti</span></div><div class="token-line" style="color:#393A34"><span class="token plain">&gt; GPU Device has SM 8.9 compute capability</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Processing time: 0.108000 (ms)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">simpleAtomicIntrinsics_nvrtc completed, returned OK</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-simpleattributes">simpleAttributes</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">./simpleAttributes Starting...</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">GPU Device 0: &quot;Ada&quot; with compute capability 8.9</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Processing time: 6674.319824 (ms)</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-simpleawbarrier">simpleAWBarrier</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">./simpleAWBarrier starting...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">GPU Device 0: &quot;Ada&quot; with compute capability 8.9</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Launching normVecByDotProductAWBarrier kernel with numBlocks = 120 blockSize = 768</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Result = PASSED</span></div><div class="token-line" style="color:#393A34"><span class="token plain">./simpleAWBarrier completed, returned OK</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-simplecallback">simpleCallback</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">Starting simpleCallback</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Found 1 CUDA capable GPUs</span></div><div class="token-line" style="color:#393A34"><span class="token plain">GPU[0] NVIDIA GeForce RTX 4070 Ti supports SM 8.9, capable GPU Callback Functions</span></div><div class="token-line" style="color:#393A34"><span class="token plain">1 GPUs available to run Callback Functions</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Starting 8 heterogeneous computing workloads</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Total of 8 workloads finished:</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Success</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-simplecooperativegroups">simpleCooperativeGroups</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Launching a single block with 64 threads...</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain"> Sum of all ranks 0..63 in threadBlockGroup is 2016 (expected 2016)</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain"> Now creating 4 groups, each of size 16 threads:</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">   Sum of all ranks 0..15 in this tiledPartition16 group is 120 (expected 120)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">   Sum of all ranks 0..15 in this tiledPartition16 group is 120 (expected 120)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">   Sum of all ranks 0..15 in this tiledPartition16 group is 120 (expected 120)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">   Sum of all ranks 0..15 in this tiledPartition16 group is 120 (expected 120)</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">...Done.</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-simplecubemaptexture">simpleCubemapTexture</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">GPU Device 0: &quot;Ada&quot; with compute capability 8.9</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">CUDA device [NVIDIA GeForce RTX 4070 Ti] has 60 Multi-Processors SM 8.9</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Covering Cubemap data array of 64~3 x 1: Grid size is 8 x 8, each block has 8 x 8 threads</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Processing time: 0.005 msec</span></div><div class="token-line" style="color:#393A34"><span class="token plain">4915.20 Mtexlookups/sec</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Comparing kernel output to expected data</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-simplecudagraphs">simpleCudaGraphs</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">GPU Device 0: &quot;Ada&quot; with compute capability 8.9</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">16777216 elements</span></div><div class="token-line" style="color:#393A34"><span class="token plain">threads per block  = 512</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Graph Launch iterations = 3</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Num of nodes in the graph created manually = 7</span></div><div class="token-line" style="color:#393A34"><span class="token plain">[cudaGraphsManual] Host callback final reduced sum = 0.996214</span></div><div class="token-line" style="color:#393A34"><span class="token plain">[cudaGraphsManual] Host callback final reduced sum = 0.996214</span></div><div class="token-line" style="color:#393A34"><span class="token plain">[cudaGraphsManual] Host callback final reduced sum = 0.996214</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Cloned Graph Output.. </span></div><div class="token-line" style="color:#393A34"><span class="token plain">[cudaGraphsManual] Host callback final reduced sum = 0.996214</span></div><div class="token-line" style="color:#393A34"><span class="token plain">[cudaGraphsManual] Host callback final reduced sum = 0.996214</span></div><div class="token-line" style="color:#393A34"><span class="token plain">[cudaGraphsManual] Host callback final reduced sum = 0.996214</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Num of nodes in the graph created using stream capture API = 7</span></div><div class="token-line" style="color:#393A34"><span class="token plain">[cudaGraphsUsingStreamCapture] Host callback final reduced sum = 0.996214</span></div><div class="token-line" style="color:#393A34"><span class="token plain">[cudaGraphsUsingStreamCapture] Host callback final reduced sum = 0.996214</span></div><div class="token-line" style="color:#393A34"><span class="token plain">[cudaGraphsUsingStreamCapture] Host callback final reduced sum = 0.996214</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Cloned Graph Output.. </span></div><div class="token-line" style="color:#393A34"><span class="token plain">[cudaGraphsUsingStreamCapture] Host callback final reduced sum = 0.996214</span></div><div class="token-line" style="color:#393A34"><span class="token plain">[cudaGraphsUsingStreamCapture] Host callback final reduced sum = 0.996214</span></div><div class="token-line" style="color:#393A34"><span class="token plain">[cudaGraphsUsingStreamCapture] Host callback final reduced sum = 0.996214</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-simplecufft_2d_mgpu">simpleCUFFT_2d_MGPU</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Poisson equation using CUFFT library on Multiple GPUs is starting...</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">No. of GPU on node 1</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Two GPUs are required to run simpleCUFFT_2d_MGPU sample code</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-simpledrvruntime">simpleDrvRuntime</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">simpleDrvRuntime..</span></div><div class="token-line" style="color:#393A34"><span class="token plain">GPU Device 0: &quot;Ada&quot; with compute capability 8.9</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">&gt; findModulePath found file at &lt;./vectorAdd_kernel64.fatbin&gt;</span></div><div class="token-line" style="color:#393A34"><span class="token plain">&gt; initCUDA loading module: &lt;./vectorAdd_kernel64.fatbin&gt;</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Result = PASS</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-simplehyperq">simpleHyperQ</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">starting hyperQ...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">GPU Device 0: &quot;Ada&quot; with compute capability 8.9</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">&gt; Detected Compute SM 8.9 hardware with 60 multi-processors</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Expected time for serial execution of 32 sets of kernels is between approx. 0.330s and 0.640s</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Expected time for fully concurrent execution of 32 sets of kernels is approx. 0.020s</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Measured time for sample = 0.058s</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-simpleipc">simpleIPC</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">Process 0: Starting on device 0...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Step 0 done</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Process 0: verifying...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Process 0 complete!</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-simplelayeredtexture">simpleLayeredTexture</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">[simpleLayeredTexture] - Starting...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">GPU Device 0: &quot;Ada&quot; with compute capability 8.9</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">CUDA device [NVIDIA GeForce RTX 4070 Ti] has 60 Multi-Processors SM 8.9</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Covering 2D data array of 512 x 512: Grid size is 64 x 64, each block has 8 x 8 threads</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Processing time: 0.024 msec</span></div><div class="token-line" style="color:#393A34"><span class="token plain">54613.33 Mtexlookups/sec</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Comparing kernel output to expected data</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-simplempi">simpleMPI</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">Invalid MIT-MAGIC-COOKIE-1 keyRunning on 1 nodes</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Average of square roots is: 0.667242</span></div><div class="token-line" style="color:#393A34"><span class="token plain">PASSED</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-simplemulticopy">simpleMultiCopy</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">[simpleMultiCopy] - Starting...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">&gt; Using CUDA device [0]: NVIDIA GeForce RTX 4070 Ti</span></div><div class="token-line" style="color:#393A34"><span class="token plain">[NVIDIA GeForce RTX 4070 Ti] has 60 MP(s) x 128 (Cores/MP) = 7680 (Cores)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">&gt; Device name: NVIDIA GeForce RTX 4070 Ti</span></div><div class="token-line" style="color:#393A34"><span class="token plain">&gt; CUDA Capability 8.9 hardware with 60 multi-processors</span></div><div class="token-line" style="color:#393A34"><span class="token plain">&gt; scale_factor = 1.00</span></div><div class="token-line" style="color:#393A34"><span class="token plain">&gt; array_size   = 4194304</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Relevant properties of this CUDA device</span></div><div class="token-line" style="color:#393A34"><span class="token plain">(X) Can overlap one CPU&lt;&gt;GPU data transfer with GPU kernel execution (device property &quot;deviceOverlap&quot;)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">(X) Can overlap two CPU&lt;&gt;GPU data transfers with GPU kernel execution</span></div><div class="token-line" style="color:#393A34"><span class="token plain">    (Compute Capability &gt;= 2.0 AND (Tesla product OR Quadro 4000/5000/6000/K5000)</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Measured timings (throughput):</span></div><div class="token-line" style="color:#393A34"><span class="token plain"> Memcpy host to device  : 0.703936 ms (23.833440 GB/s)</span></div><div class="token-line" style="color:#393A34"><span class="token plain"> Memcpy device to host  : 0.640000 ms (26.214401 GB/s)</span></div><div class="token-line" style="color:#393A34"><span class="token plain"> Kernel         : 0.037728 ms (4446.887142 GB/s)</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Theoretical limits for speedup gained from overlapped data transfers:</span></div><div class="token-line" style="color:#393A34"><span class="token plain">No overlap at all (transfer-kernel-transfer): 1.381664 ms </span></div><div class="token-line" style="color:#393A34"><span class="token plain">Compute can overlap with one transfer: 1.343936 ms</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Compute can overlap with both data transfers: 0.703936 ms</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Average measured timings over 10 repetitions:</span></div><div class="token-line" style="color:#393A34"><span class="token plain"> Avg. time when execution fully serialized  : 1.389363 ms</span></div><div class="token-line" style="color:#393A34"><span class="token plain"> Avg. time when overlapped using 4 streams  : 0.751098 ms</span></div><div class="token-line" style="color:#393A34"><span class="token plain"> Avg. speedup gained (serialized - overlapped)  : 0.638266 ms</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Measured throughput:</span></div><div class="token-line" style="color:#393A34"><span class="token plain"> Fully serialized execution     : 24.150944 GB/s</span></div><div class="token-line" style="color:#393A34"><span class="token plain"> Overlapped using 4 streams     : 44.673865 GB/s</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-simplemultigpu">simpleMultiGPU</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">Starting simpleMultiGPU</span></div><div class="token-line" style="color:#393A34"><span class="token plain">CUDA-capable device count: 1</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Generating input data...</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Computing with 1 GPUs...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">  GPU Processing time: 6.476000 (ms)</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Computing with Host CPU...</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Comparing GPU and Host CPU results...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">  GPU sum: 16777296.000000</span></div><div class="token-line" style="color:#393A34"><span class="token plain">  CPU sum: 16777294.395033</span></div><div class="token-line" style="color:#393A34"><span class="token plain">  Relative difference: 9.566307E-08 </span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-simpleoccupancy">simpleOccupancy</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">starting Simple Occupancy</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">[ Manual configuration with 32 threads per block ]</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Potential occupancy: 50%</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Elapsed time: 0.054464ms</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">[ Automatic, occupancy-based configuration ]</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Suggested block size: 768</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Minimum grid size for maximum occupancy: 120</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Potential occupancy: 100%</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Elapsed time: 0.008512ms</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Test PASSED</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-simplep2p">simpleP2P</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">[./simpleP2P] - Starting...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Checking for multiple GPUs...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">CUDA-capable device count: 1</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Two or more GPUs with Peer-to-Peer access capability are required for ./simpleP2P.</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Waiving test.</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-simplepitchlineartexture">simplePitchLinearTexture</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">simplePitchLinearTexture starting...</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">GPU Device 0: &quot;Ada&quot; with compute capability 8.9</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Bandwidth (GB/s) for pitch linear: 1.40e+03; for array: 1.83e+03</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Texture fetch rate (Mpix/s) for pitch linear: 1.75e+05; for array: 2.29e+05</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">simplePitchLinearTexture completed, returned OK</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-simpleprintf">simplePrintf</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">GPU Device 0: &quot;Ada&quot; with compute capability 8.9</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Device 0: &quot;NVIDIA GeForce RTX 4070 Ti&quot; with Compute 8.9 capability</span></div><div class="token-line" style="color:#393A34"><span class="token plain">printf() is called. Output:</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">[2, 0]:     Value is:10</span></div><div class="token-line" style="color:#393A34"><span class="token plain">[2, 1]:     Value is:10</span></div><div class="token-line" style="color:#393A34"><span class="token plain">[2, 2]:     Value is:10</span></div><div class="token-line" style="color:#393A34"><span class="token plain">[2, 3]:     Value is:10</span></div><div class="token-line" style="color:#393A34"><span class="token plain">[2, 4]:     Value is:10</span></div><div class="token-line" style="color:#393A34"><span class="token plain">[2, 5]:     Value is:10</span></div><div class="token-line" style="color:#393A34"><span class="token plain">[2, 6]:     Value is:10</span></div><div class="token-line" style="color:#393A34"><span class="token plain">[2, 7]:     Value is:10</span></div><div class="token-line" style="color:#393A34"><span class="token plain">[3, 0]:     Value is:10</span></div><div class="token-line" style="color:#393A34"><span class="token plain">[3, 1]:     Value is:10</span></div><div class="token-line" style="color:#393A34"><span class="token plain">[3, 2]:     Value is:10</span></div><div class="token-line" style="color:#393A34"><span class="token plain">[3, 3]:     Value is:10</span></div><div class="token-line" style="color:#393A34"><span class="token plain">[3, 4]:     Value is:10</span></div><div class="token-line" style="color:#393A34"><span class="token plain">[3, 5]:     Value is:10</span></div><div class="token-line" style="color:#393A34"><span class="token plain">[3, 6]:     Value is:10</span></div><div class="token-line" style="color:#393A34"><span class="token plain">[3, 7]:     Value is:10</span></div><div class="token-line" style="color:#393A34"><span class="token plain">[1, 0]:     Value is:10</span></div><div class="token-line" style="color:#393A34"><span class="token plain">[1, 1]:     Value is:10</span></div><div class="token-line" style="color:#393A34"><span class="token plain">[1, 2]:     Value is:10</span></div><div class="token-line" style="color:#393A34"><span class="token plain">[1, 3]:     Value is:10</span></div><div class="token-line" style="color:#393A34"><span class="token plain">[1, 4]:     Value is:10</span></div><div class="token-line" style="color:#393A34"><span class="token plain">[1, 5]:     Value is:10</span></div><div class="token-line" style="color:#393A34"><span class="token plain">[1, 6]:     Value is:10</span></div><div class="token-line" style="color:#393A34"><span class="token plain">[1, 7]:     Value is:10</span></div><div class="token-line" style="color:#393A34"><span class="token plain">[0, 0]:     Value is:10</span></div><div class="token-line" style="color:#393A34"><span class="token plain">[0, 1]:     Value is:10</span></div><div class="token-line" style="color:#393A34"><span class="token plain">[0, 2]:     Value is:10</span></div><div class="token-line" style="color:#393A34"><span class="token plain">[0, 3]:     Value is:10</span></div><div class="token-line" style="color:#393A34"><span class="token plain">[0, 4]:     Value is:10</span></div><div class="token-line" style="color:#393A34"><span class="token plain">[0, 5]:     Value is:10</span></div><div class="token-line" style="color:#393A34"><span class="token plain">[0, 6]:     Value is:10</span></div><div class="token-line" style="color:#393A34"><span class="token plain">[0, 7]:     Value is:10</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-simpleseparatecompilation">simpleSeparateCompilation</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">simpleSeparateCompilation starting...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">GPU Device 0: &quot;Ada&quot; with compute capability 8.9</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">simpleSeparateCompilation completed, returned OK</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-simplestreams">simpleStreams</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">[ simpleStreams ]</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Device synchronization method set to = 0 (Automatic Blocking)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Setting reps to 100 to demonstrate steady state</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">&gt; GPU Device 0: &quot;Ada&quot; with compute capability 8.9</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Device: &lt;NVIDIA GeForce RTX 4070 Ti&gt; canMapHostMemory: Yes</span></div><div class="token-line" style="color:#393A34"><span class="token plain">&gt; CUDA Capable: SM 8.9 hardware</span></div><div class="token-line" style="color:#393A34"><span class="token plain">&gt; 60 Multiprocessor(s) x 128 (Cores/Multiprocessor) = 7680 (Cores)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">&gt; scale_factor = 1.0000</span></div><div class="token-line" style="color:#393A34"><span class="token plain">&gt; array_size   = 16777216</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">&gt; Using CPU/GPU Device Synchronization method (cudaDeviceScheduleAuto)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">&gt; mmap() allocating 64.00 Mbytes (generic page-aligned system memory)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">&gt; cudaHostRegister() registering 64.00 Mbytes of generic allocated system memory</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Starting Test</span></div><div class="token-line" style="color:#393A34"><span class="token plain">memcopy:    2.55</span></div><div class="token-line" style="color:#393A34"><span class="token plain">kernel:     0.31</span></div><div class="token-line" style="color:#393A34"><span class="token plain">non-streamed:   2.79</span></div><div class="token-line" style="color:#393A34"><span class="token plain">4 streams:  2.64</span></div><div class="token-line" style="color:#393A34"><span class="token plain">-------------------------------</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-simplesurfacewrite">simpleSurfaceWrite</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">simpleSurfaceWrite starting...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">GPU Device 0: &quot;Ada&quot; with compute capability 8.9</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">CUDA device [NVIDIA GeForce RTX 4070 Ti] has 60 Multi-Processors, SM 8.9</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Loaded &#x27;teapot512.pgm&#x27;, 512 x 512 pixels</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Processing time: 0.007000 (ms)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">37449.14 Mpixels/sec</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Wrote &#x27;output.pgm&#x27;</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Comparing files</span></div><div class="token-line" style="color:#393A34"><span class="token plain">    output:    &lt;output.pgm&gt;</span></div><div class="token-line" style="color:#393A34"><span class="token plain">    reference: &lt;../../../../Samples/0_Introduction/simpleSurfaceWrite/data/ref_rotated.pgm&gt;</span></div><div class="token-line" style="color:#393A34"><span class="token plain">simpleSurfaceWrite completed, returned OK</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-simpletemplates">simpleTemplates</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">&gt; runTest&lt;float,32&gt;</span></div><div class="token-line" style="color:#393A34"><span class="token plain">GPU Device 0: &quot;Ada&quot; with compute capability 8.9</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">CUDA device [NVIDIA GeForce RTX 4070 Ti] has 60 Multi-Processors</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Processing time: 0.119000 (ms)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Compare OK</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">&gt; runTest&lt;int,64&gt;</span></div><div class="token-line" style="color:#393A34"><span class="token plain">GPU Device 0: &quot;Ada&quot; with compute capability 8.9</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">CUDA device [NVIDIA GeForce RTX 4070 Ti] has 60 Multi-Processors</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Processing time: 0.043000 (ms)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Compare OK</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">[simpleTemplates] -&gt; Test Results: 0 Failures</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-simpletemplates_nvrtc">simpleTemplates_nvrtc</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">&gt; runTest&lt;float,32&gt;</span></div><div class="token-line" style="color:#393A34"><span class="token plain">&gt; Using CUDA Device [0]: NVIDIA GeForce RTX 4070 Ti</span></div><div class="token-line" style="color:#393A34"><span class="token plain">&gt; Using CUDA Device [0]: NVIDIA GeForce RTX 4070 Ti</span></div><div class="token-line" style="color:#393A34"><span class="token plain">&gt; GPU Device has SM 8.9 compute capability</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Processing time: 0.064000 (ms)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Compare OK</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">&gt; runTest&lt;int,64&gt;</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Processing time: 0.050000 (ms)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Compare OK</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">[simpleTemplates_nvrtc] -&gt; Test Results: 0 Failures</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-simpletexture">simpleTexture</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">simpleTexture starting...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">GPU Device 0: &quot;Ada&quot; with compute capability 8.9</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Loaded &#x27;teapot512.pgm&#x27;, 512 x 512 pixels</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Processing time: 0.007000 (ms)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">37449.14 Mpixels/sec</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Wrote &#x27;../../../../Samples/0_Introduction/simpleTexture/data/teapot512_out.pgm&#x27;</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Comparing files</span></div><div class="token-line" style="color:#393A34"><span class="token plain">    output:    &lt;../../../../Samples/0_Introduction/simpleTexture/data/teapot512_out.pgm&gt;</span></div><div class="token-line" style="color:#393A34"><span class="token plain">    reference: &lt;../../../../Samples/0_Introduction/simpleTexture/data/ref_rotated.pgm&gt;</span></div><div class="token-line" style="color:#393A34"><span class="token plain">simpleTexture completed, returned OK</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-simpletexturedrv">simpleTextureDrv</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">&gt; Using CUDA Device [0]: NVIDIA GeForce RTX 4070 Ti</span></div><div class="token-line" style="color:#393A34"><span class="token plain">&gt; GPU Device has SM 8.9 compute capability</span></div><div class="token-line" style="color:#393A34"><span class="token plain">&gt; findModulePath found file at &lt;./simpleTexture_kernel64.fatbin&gt;</span></div><div class="token-line" style="color:#393A34"><span class="token plain">&gt; initCUDA loading module: &lt;./simpleTexture_kernel64.fatbin&gt;</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Loaded &#x27;teapot512.pgm&#x27;, 512 x 512 pixels</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Processing time: 0.007000 (ms)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">37449.14 Mpixels/sec</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Wrote &#x27;../../../../Samples/0_Introduction/simpleTextureDrv/data/teapot512_out.pgm&#x27;</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Comparing files</span></div><div class="token-line" style="color:#393A34"><span class="token plain">    output:    &lt;../../../../Samples/0_Introduction/simpleTextureDrv/data/teapot512_out.pgm&gt;</span></div><div class="token-line" style="color:#393A34"><span class="token plain">    reference: &lt;../../../../Samples/0_Introduction/simpleTextureDrv/data/ref_rotated.pgm&gt;</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-simplevoteintrinsics">simpleVoteIntrinsics</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">[simpleVoteIntrinsics]</span></div><div class="token-line" style="color:#393A34"><span class="token plain">GPU Device 0: &quot;Ada&quot; with compute capability 8.9</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">&gt; GPU device has 60 Multi-Processors, SM 8.9 compute capabilities</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">[VOTE Kernel Test 1/3]</span></div><div class="token-line" style="color:#393A34"><span class="token plain">    Running &lt;&lt;Vote.Any&gt;&gt; kernel1 ...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">    OK</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">[VOTE Kernel Test 2/3]</span></div><div class="token-line" style="color:#393A34"><span class="token plain">    Running &lt;&lt;Vote.All&gt;&gt; kernel2 ...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">    OK</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">[VOTE Kernel Test 3/3]</span></div><div class="token-line" style="color:#393A34"><span class="token plain">    Running &lt;&lt;Vote.Any&gt;&gt; kernel3 ...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">    OK</span></div><div class="token-line" style="color:#393A34"><span class="token plain">    Shutting down...</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-simplevoteintrinsics_nvrtc">simpleVoteIntrinsics_nvrtc</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">&gt; Using CUDA Device [0]: NVIDIA GeForce RTX 4070 Ti</span></div><div class="token-line" style="color:#393A34"><span class="token plain">&gt; Using CUDA Device [0]: NVIDIA GeForce RTX 4070 Ti</span></div><div class="token-line" style="color:#393A34"><span class="token plain">&gt; GPU Device has SM 8.9 compute capability</span></div><div class="token-line" style="color:#393A34"><span class="token plain">[simpleVoteIntrinsics_nvrtc]</span></div><div class="token-line" style="color:#393A34"><span class="token plain">[VOTE Kernel Test 1/3]</span></div><div class="token-line" style="color:#393A34"><span class="token plain">    Running &lt;&lt;Vote.Any&gt;&gt; kernel1 ...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">    OK</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">[VOTE Kernel Test 2/3]</span></div><div class="token-line" style="color:#393A34"><span class="token plain">    Running &lt;&lt;Vote.All&gt;&gt; kernel2 ...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">    OK</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">[VOTE Kernel Test 3/3]</span></div><div class="token-line" style="color:#393A34"><span class="token plain">    Running &lt;&lt;Vote.Any&gt;&gt; kernel3 ...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">    OK</span></div><div class="token-line" style="color:#393A34"><span class="token plain">    Shutting down...</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-simplezerocopy">simpleZeroCopy</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">  Device 0: &lt;             Ada &gt;, Compute SM 8.9 detected</span></div><div class="token-line" style="color:#393A34"><span class="token plain">&gt; Using CUDA Host Allocated (cudaHostAlloc)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">&gt; vectorAddGPU kernel will add vectors using mapped CPU memory...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">&gt; Checking the results from vectorAddGPU() ...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">&gt; Releasing CPU memory...</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-sobolqrng">SobolQRNG</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">Sobol Quasi-Random Number Generator Starting...</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">&gt; number of vectors = 100000</span></div><div class="token-line" style="color:#393A34"><span class="token plain">&gt; number of dimensions = 100</span></div><div class="token-line" style="color:#393A34"><span class="token plain">GPU Device 0: &quot;Ada&quot; with compute capability 8.9</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Allocating CPU memory...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Allocating GPU memory...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Initializing direction numbers...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Copying direction numbers to device...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Executing QRNG on GPU...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Gsamples/s: 52.9101</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Reading results from GPU...</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Executing QRNG on CPU...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Gsamples/s: 0.448853</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Checking results...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">L1-Error: 0</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Shutting down...</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-stereodisparity">stereoDisparity</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">[stereoDisparity] Starting...</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">GPU Device 0: &quot;Ada&quot; with compute capability 8.9</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">&gt; GPU device has 60 Multi-Processors, SM 8.9 compute capabilities</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Loaded &lt;../../../../Samples/5_Domain_Specific/stereoDisparity/data/stereo.im0.640x533.ppm&gt; as image 0</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Loaded &lt;../../../../Samples/5_Domain_Specific/stereoDisparity/data/stereo.im1.640x533.ppm&gt; as image 1</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Launching CUDA stereoDisparityKernel()</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Input Size  [640x533], Kernel size [17x17], Disparities [-16:0]</span></div><div class="token-line" style="color:#393A34"><span class="token plain">GPU processing time : 0.1874 (ms)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Pixel throughput    : 1820.355 Mpixels/sec</span></div><div class="token-line" style="color:#393A34"><span class="token plain">GPU Checksum = 4293895789, GPU image: &lt;output_GPU.pgm&gt;</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Computing CPU reference...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">CPU Checksum = 4293895789, CPU image: &lt;output_CPU.pgm&gt;</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-streampriorities">StreamPriorities</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">Starting [./StreamPriorities]...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">GPU Device 0: &quot;Ada&quot; with compute capability 8.9</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">CUDA stream priority range: LOW: 0 to HIGH: -5</span></div><div class="token-line" style="color:#393A34"><span class="token plain">elapsed time of kernels launched to LOW priority stream: 2.885 ms</span></div><div class="token-line" style="color:#393A34"><span class="token plain">elapsed time of kernels launched to HI  priority stream: 1.838 ms</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-systemwideatomics">systemWideAtomics</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">GPU Device 0: &quot;Ada&quot; with compute capability 8.9</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">CANNOT access pageable memory</span></div><div class="token-line" style="color:#393A34"><span class="token plain">systemWideAtomics completed, returned OK </span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-template">template</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">./template Starting...</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">GPU Device 0: &quot;Ada&quot; with compute capability 8.9</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Processing time: 0.108000 (ms)</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-tf32tensorcoregemm">tf32TensorCoreGemm</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">Initializing...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">GPU Device 0: &quot;Ada&quot; with compute capability 8.9</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">M: 8192 (16 x 512)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">N: 8192 (16 x 512)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">K: 4096 (8 x 512)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Preparing data for GPU...</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Required shared memory size: 72 Kb</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Computing using high performance kernel = 0 - compute_tf32gemm_async_copy</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Time: 80.129021 ms</span></div><div class="token-line" style="color:#393A34"><span class="token plain">TFLOPS: 6.86</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-topologyquery">topologyQuery</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">GPU0 &lt;-&gt; CPU:</span></div><div class="token-line" style="color:#393A34"><span class="token plain">  * Atomic Supported: no</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-unifiedmemorystreams">UnifiedMemoryStreams</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">GPU Device 0: &quot;Ada&quot; with compute capability 8.9</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Executing tasks on host / device</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Task [2], thread [0] executing on device (368)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Task [0], thread [2] executing on device (884)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Task [3], thread [1] executing on host (64)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Task [1], thread [3] executing on device (387)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Task [4], thread [1] executing on device (250)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Task [5], thread [0] executing on device (399)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Task [6], thread [2] executing on device (131)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Task [7], thread [3] executing on device (642)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Task [8], thread [1] executing on device (704)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Task [9], thread [0] executing on device (469)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Task [10], thread [0] executing on device (174)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Task [11], thread [1] executing on device (286)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Task [12], thread [1] executing on device (513)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Task [13], thread [0] executing on device (789)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Task [14], thread [0] executing on device (604)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Task [15], thread [1] executing on device (133)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Task [16], thread [0] executing on device (795)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Task [17], thread [3] executing on device (578)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Task [18], thread [2] executing on host (91)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Task [19], thread [1] executing on device (592)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Task [20], thread [1] executing on device (426)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Task [21], thread [1] executing on host (64)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Task [22], thread [1] executing on device (279)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Task [23], thread [0] executing on device (990)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Task [24], thread [1] executing on device (160)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Task [25], thread [0] executing on device (644)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Task [26], thread [0] executing on device (830)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Task [27], thread [0] executing on host (64)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Task [28], thread [2] executing on device (877)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Task [29], thread [2] executing on device (523)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Task [30], thread [0] executing on device (834)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Task [31], thread [0] executing on device (485)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Task [32], thread [2] executing on host (64)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Task [33], thread [2] executing on device (577)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Task [34], thread [2] executing on device (781)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Task [35], thread [2] executing on device (879)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Task [36], thread [2] executing on device (564)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Task [37], thread [2] executing on device (802)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Task [38], thread [2] executing on device (389)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Task [39], thread [2] executing on device (954)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">All Done!</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-vectoradd">vectorAdd</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">[Vector addition of 50000 elements]</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Copy input data from the host memory to the CUDA device</span></div><div class="token-line" style="color:#393A34"><span class="token plain">CUDA kernel launch with 196 blocks of 256 threads</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Copy output data from the CUDA device to the host memory</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Test PASSED</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Done</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-vectoradddrv">vectorAddDrv</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">Vector Addition (Driver API)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">&gt; Using CUDA Device [0]: NVIDIA GeForce RTX 4070 Ti</span></div><div class="token-line" style="color:#393A34"><span class="token plain">&gt; findModulePath found file at &lt;./vectorAdd_kernel64.fatbin&gt;</span></div><div class="token-line" style="color:#393A34"><span class="token plain">&gt; initCUDA loading module: &lt;./vectorAdd_kernel64.fatbin&gt;</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Result = PASS</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-vectoraddmmap">vectorAddMMAP</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">Vector Addition (Driver API)</span></div><div class="token-line" style="color:#393A34"><span class="token plain">&gt; Using CUDA Device [0]: NVIDIA GeForce RTX 4070 Ti</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Device 0 VIRTUAL ADDRESS MANAGEMENT SUPPORTED = 1.</span></div><div class="token-line" style="color:#393A34"><span class="token plain">&gt; findModulePath found file at &lt;./vectorAdd_kernel64.fatbin&gt;</span></div><div class="token-line" style="color:#393A34"><span class="token plain">&gt; initCUDA loading module: &lt;./vectorAdd_kernel64.fatbin&gt;</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Result = PASS</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-vectoradd_nvrtc">vectorAdd_nvrtc</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">&gt; Using CUDA Device [0]: NVIDIA GeForce RTX 4070 Ti</span></div><div class="token-line" style="color:#393A34"><span class="token plain">&gt; Using CUDA Device [0]: NVIDIA GeForce RTX 4070 Ti</span></div><div class="token-line" style="color:#393A34"><span class="token plain">&gt; GPU Device has SM 8.9 compute capability</span></div><div class="token-line" style="color:#393A34"><span class="token plain">[Vector addition of 50000 elements]</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Copy input data from the host memory to the CUDA device</span></div><div class="token-line" style="color:#393A34"><span class="token plain">CUDA kernel launch with 196 blocks of 256 threads</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Copy output data from the CUDA device to the host memory</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Test PASSED</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Done</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div><h1 id="-warpaggregatedatomicscg">warpAggregatedAtomicsCG</h1><div class="relative"><pre class="prism-code language-" style="color:#393A34;background-color:#f6f8fa"><div class="token-line" style="color:#393A34"><span class="token plain">GPU Device 0: &quot;Ada&quot; with compute capability 8.9</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">CPU max matches GPU max</span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#393A34"><span class="token plain">Warp Aggregated Atomics PASSED </span></div><div class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></div></pre></div></article></div></div><div class="flex py-2 text-xs flex-wrap justify-between items-center ml:0 md:ml-14"></div></div><div class="w-full md:w-3/12 md:pl-4"><div class="pb-4 border-b"><h2 class="mb-4 font-medium">About</h2><a class="flex items-center text-gray-medium hover:text-blue hover:no-underline mb-4" href="/categories/nvidia"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 576 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M527.9 224H480v-48c0-26.5-21.5-48-48-48H272l-64-64H48C21.5 64 0 85.5 0 112v288c0 26.5 21.5 48 48 48h400c16.5 0 31.9-8.5 40.7-22.6l79.9-128c20-31.9-3-73.4-40.7-73.4zM48 118c0-3.3 2.7-6 6-6h134.1l64 64H426c3.3 0 6 2.7 6 6v42H152c-16.8 0-32.4 8.8-41.1 23.2L48 351.4zm400 282H72l77.2-128H528z"></path></svg><span class="ml-3 text-sm">Nvidia</span></a><div><a class="topic-tag" href="/tags/nvidia">Nvidia<!-- --> <span class="font-semibold"></span></a></div></div><div class="pb-4 border-b sticky top-0"><h2 class="my-4 font-medium">Table of Content</h2><ul class="text-sm mb-2 "><li><a href="#asyncapi" class="">asyncAPI</a></li><li><a href="#bandwidthtest" class="">bandwidthTest</a></li><li><a href="#batchcublas" class="">batchCUBLAS</a></li><li><a href="#bf16tensorcoregemm" class="">bf16TensorCoreGemm</a></li><li><a href="#binarypartitioncg" class="">binaryPartitionCG</a></li><li><a href="#binomialoptions" class="">binomialOptions</a></li><li><a href="#binomialoptions_nvrtc" class="">binomialOptions_nvrtc</a></li><li><a href="#blackscholes" class="">BlackScholes</a></li><li><a href="#blackscholes_nvrtc" class="">BlackScholes_nvrtc</a></li><li><a href="#c11_cuda" class="">c++11_cuda</a></li><li><a href="#cdpadvancedquicksort" class="">cdpAdvancedQuicksort</a></li><li><a href="#cdpbeziertessellation" class="">cdpBezierTessellation</a></li><li><a href="#cdpquadtree" class="">cdpQuadtree</a></li><li><a href="#cdpsimpleprint" class="">cdpSimplePrint</a></li><li><a href="#cdpsimplequicksort" class="">cdpSimpleQuicksort</a></li><li><a href="#clock" class="">clock</a></li><li><a href="#clock_nvrtc" class="">clock_nvrtc</a></li><li><a href="#concurrentkernels" class="">concurrentKernels</a></li><li><a href="#conjugategradientmultidevicecg" class="">conjugateGradientMultiDeviceCG</a></li><li><a href="#convolutionfft2d" class="">convolutionFFT2D</a></li><li><a href="#cppintegration" class="">cppIntegration</a></li><li><a href="#cppoverload" class="">cppOverload</a></li><li><a href="#cudacompressiblememory" class="">cudaCompressibleMemory</a></li><li><a href="#cudaopenmp" class="">cudaOpenMP</a></li><li><a href="#cudatensorcoregemm" class="">cudaTensorCoreGemm</a></li><li><a href="#devicequery" class="">deviceQuery</a></li><li><a href="#devicequerydrv" class="">deviceQueryDrv</a></li><li><a href="#dmmatensorcoregemm" class="">dmmaTensorCoreGemm</a></li><li><a href="#dwthaar1d" class="">dwtHaar1D</a></li><li><a href="#dxtc" class="">dxtc</a></li><li><a href="#encode_output" class="">encode_output</a></li><li><a href="#fastwalshtransform" class="">fastWalshTransform</a></li><li><a href="#fdtd3d" class="">FDTD3d</a></li><li><a href="#fp16scalarproduct" class="">fp16ScalarProduct</a></li><li><a href="#globaltoshmemasynccopy" class="">globalToShmemAsyncCopy</a></li><li><a href="#graphmemoryfootprint" class="">graphMemoryFootprint</a></li><li><a href="#graphmemorynodes" class="">graphMemoryNodes</a></li><li><a href="#hsopticalflow" class="">HSOpticalFlow</a></li><li><a href="#immatensorcoregemm" class="">immaTensorCoreGemm</a></li><li><a href="#jacobicudagraphs" class="">jacobiCudaGraphs</a></li><li><a href="#matrixmul" class="">matrixMul</a></li><li><a href="#matrixmuldrv" class="">matrixMulDrv</a></li><li><a href="#matrixmuldynlinkjit" class="">matrixMulDynlinkJIT</a></li><li><a href="#matrixmul_nvrtc" class="">matrixMul_nvrtc</a></li><li><a href="#memmapipcdrv" class="">memMapIPCDrv</a></li><li><a href="#mergesort" class="">mergeSort</a></li><li><a href="#montecarlomultigpu" class="">MonteCarloMultiGPU</a></li><li><a href="#newdelete" class="">newdelete</a></li><li><a href="#nv12tobgrandresize" class="">NV12toBGRandResize</a></li><li><a href="#output" class="">output</a></li><li><a href="#p2pbandwidthlatencytest" class="">p2pBandwidthLatencyTest</a></li><li><a href="#ptxjit" class="">ptxjit</a></li><li><a href="#quasirandomgenerator" class="">quasirandomGenerator</a></li><li><a href="#quasirandomgenerator_nvrtc" class="">quasirandomGenerator_nvrtc</a></li><li><a href="#simpleassert" class="">simpleAssert</a></li><li><a href="#simpleassert_nvrtc" class="">simpleAssert_nvrtc</a></li><li><a href="#simpleatomicintrinsics" class="">simpleAtomicIntrinsics</a></li><li><a href="#simpleatomicintrinsics_nvrtc" class="">simpleAtomicIntrinsics_nvrtc</a></li><li><a href="#simpleattributes" class="">simpleAttributes</a></li><li><a href="#simpleawbarrier" class="">simpleAWBarrier</a></li><li><a href="#simplecallback" class="">simpleCallback</a></li><li><a href="#simplecooperativegroups" class="">simpleCooperativeGroups</a></li><li><a href="#simplecubemaptexture" class="">simpleCubemapTexture</a></li><li><a href="#simplecudagraphs" class="">simpleCudaGraphs</a></li><li><a href="#simplecufft_2d_mgpu" class="">simpleCUFFT_2d_MGPU</a></li><li><a href="#simpledrvruntime" class="">simpleDrvRuntime</a></li><li><a href="#simplehyperq" class="">simpleHyperQ</a></li><li><a href="#simpleipc" class="">simpleIPC</a></li><li><a href="#simplelayeredtexture" class="">simpleLayeredTexture</a></li><li><a href="#simplempi" class="">simpleMPI</a></li><li><a href="#simplemulticopy" class="">simpleMultiCopy</a></li><li><a href="#simplemultigpu" class="">simpleMultiGPU</a></li><li><a href="#simpleoccupancy" class="">simpleOccupancy</a></li><li><a href="#simplep2p" class="">simpleP2P</a></li><li><a href="#simplepitchlineartexture" class="">simplePitchLinearTexture</a></li><li><a href="#simpleprintf" class="">simplePrintf</a></li><li><a href="#simpleseparatecompilation" class="">simpleSeparateCompilation</a></li><li><a href="#simplestreams" class="">simpleStreams</a></li><li><a href="#simplesurfacewrite" class="">simpleSurfaceWrite</a></li><li><a href="#simpletemplates" class="">simpleTemplates</a></li><li><a href="#simpletemplates_nvrtc" class="">simpleTemplates_nvrtc</a></li><li><a href="#simpletexture" class="">simpleTexture</a></li><li><a href="#simpletexturedrv" class="">simpleTextureDrv</a></li><li><a href="#simplevoteintrinsics" class="">simpleVoteIntrinsics</a></li><li><a href="#simplevoteintrinsics_nvrtc" class="">simpleVoteIntrinsics_nvrtc</a></li><li><a href="#simplezerocopy" class="">simpleZeroCopy</a></li><li><a href="#sobolqrng" class="">SobolQRNG</a></li><li><a href="#stereodisparity" class="">stereoDisparity</a></li><li><a href="#streampriorities" class="">StreamPriorities</a></li><li><a href="#systemwideatomics" class="">systemWideAtomics</a></li><li><a href="#template" class="">template</a></li><li><a href="#tf32tensorcoregemm" class="">tf32TensorCoreGemm</a></li><li><a href="#topologyquery" class="">topologyQuery</a></li><li><a href="#unifiedmemorystreams" class="">UnifiedMemoryStreams</a></li><li><a href="#vectoradd" class="">vectorAdd</a></li><li><a href="#vectoradddrv" class="">vectorAddDrv</a></li><li><a href="#vectoraddmmap" class="">vectorAddMMAP</a></li><li><a href="#vectoradd_nvrtc" class="">vectorAdd_nvrtc</a></li><li><a href="#warpaggregatedatomicscg" class="">warpAggregatedAtomicsCG</a></li></ul></div></div></div><footer class="border-t mt-10 pt-10 mb-4 md:mb-10"><div class="page-grid flex flex-wrap items-center md:justify-between text-xs md:flex-row"><ul class="flex flex-wrap justify-between w-full md:w-5/12 md:justify-start -mx-2 md:-mx-4"><li class="mx-2 md:mx-4">© <!-- -->2023<!-- --> <!-- -->abelleeye</li><li>built 1820 days</li><li class="mx-2 md:mx-4"><a href="https://www.abellee.cn" target="_blank">Home</a></li><li class="mx-2 md:mx-4"><a href="https://github.com/abelleeye" target="_blank">Github</a></li></ul><div class="hidden md:block md:2/12"><a class="sc-49bqba-0 jgPWsE" href="/"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" size="32" height="32" width="32" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></div><a style="display:block;width:10px;height:10px;background-color:#f9f9f9;border-radius:50%" href="/categories/nothing" target="_blank"></a></div></footer></div></div><div id="gatsby-announcer" style="position:absolute;top:0;width:1px;height:1px;padding:0;overflow:hidden;clip:rect(0, 0, 0, 0);white-space:nowrap;border:0" aria-live="assertive" aria-atomic="true"></div></div><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-52574938-2"></script><script>
      
      function gaOptout(){document.cookie=disableStr+'=true; expires=Thu, 31 Dec 2099 23:59:59 UTC;path=/',window[disableStr]=!0}var gaProperty='UA-52574938-2',disableStr='ga-disable-'+gaProperty;document.cookie.indexOf(disableStr+'=true')>-1&&(window[disableStr]=!0);
      if(!(navigator.doNotTrack == "1" || window.doNotTrack == "1")) {
        window.dataLayer = window.dataLayer || [];
        function gtag(){window.dataLayer && window.dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'UA-52574938-2', {"anonymize_ip":true,"cookie_expires":0,"send_page_view":false});
      }
      </script><script id="gatsby-script-loader">/*<![CDATA[*/window.pagePath="/2023/11/04/nvidia_4070TI_cuda_report/";/*]]>*/</script><script id="gatsby-chunk-mapping">/*<![CDATA[*/window.___chunkMapping={"polyfill":["/polyfill-2906ba4787031c3e897f.js"],"app":["/app-c2e304ea278d1bbd77b8.js"],"component---gatsby-theme-replica-src-templates-archive-tsx":["/component---gatsby-theme-replica-src-templates-archive-tsx-d1b5d9404c68e62f7375.js"],"component---gatsby-theme-replica-src-templates-categories-tsx":["/component---gatsby-theme-replica-src-templates-categories-tsx-aa5b1507db6bec9c2f91.js"],"component---gatsby-theme-replica-src-templates-category-tsx":["/component---gatsby-theme-replica-src-templates-category-tsx-8d403309851dd9314666.js"],"component---gatsby-theme-replica-src-templates-home-tsx":["/component---gatsby-theme-replica-src-templates-home-tsx-db7e15c6884b87e889e8.js"],"component---gatsby-theme-replica-src-templates-post-tsx":["/component---gatsby-theme-replica-src-templates-post-tsx-010a4a90954a64823958.js"],"component---gatsby-theme-replica-src-templates-tag-tsx":["/component---gatsby-theme-replica-src-templates-tag-tsx-6a797cb88e6a4b815497.js"],"component---gatsby-theme-replica-src-templates-tags-tsx":["/component---gatsby-theme-replica-src-templates-tags-tsx-9e2ae94c069ad8680e7e.js"],"component---src-pages-about-js":["/component---src-pages-about-js-4559f8c506ffc97fb7a8.js"]};/*]]>*/</script><script src="/polyfill-2906ba4787031c3e897f.js" nomodule=""></script><script src="/component---gatsby-theme-replica-src-templates-post-tsx-010a4a90954a64823958.js" async=""></script><script src="/aacb0a1bb7a50c0bda0baac92d46406327a248f1-1bd85d960f2e5a815fd6.js" async=""></script><script src="/43ccf438-9f3768ab58fc48b692cf.js" async=""></script><script src="/b3ead0f8-e70b62a53986d7cc5d98.js" async=""></script><script src="/e2070dfd-b06a1c5dc488eaac3595.js" async=""></script><script src="/aa642b0a-84f2665e5b07251329ab.js" async=""></script><script src="/a490077e-d0b9734bbaaad323dc86.js" async=""></script><script src="/e5908a55-3d0292d81ee122ce0f53.js" async=""></script><script src="/fa5f8105-cb81053d7202d517e9c0.js" async=""></script><script src="/abba1267-4db10b0857c7c5274881.js" async=""></script><script src="/app-c2e304ea278d1bbd77b8.js" async=""></script><script src="/styles-4c7d46604086483aa196.js" async=""></script><script src="/framework-303b5b995007c0324953.js" async=""></script><script src="/webpack-runtime-c8d2f823f313dc640930.js" async=""></script></body></html>